{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inferring_Soft_Biometrics_blind_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YxqDgUtU9u9d",
        "Et-Q1P2goy6u",
        "--WVcmG0sAtF",
        "le4kmoFWySUs"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niCXFdtm3DxP",
        "colab_type": "text"
      },
      "source": [
        "# **Loading Google drive, Sanity Checks and Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZqfXJH7g2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS63A2wHrYVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/BBMAS_Keystrokes_only/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTad3CUg2-G7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Opc0OrTRTwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install KDEpy\n",
        "!pip install pymrmr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVV7vX5SrvJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "from KDEpy import FFTKDE, TreeKDE\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import seaborn as sns\n",
        "from math import sqrt\n",
        "from scipy.stats import gaussian_kde\n",
        "from operator import itemgetter\n",
        "import shutil\n",
        "import math\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from scipy.stats import kurtosis,skew\n",
        "import xgboost as xgb\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pymrmr\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import *\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imdIwnU7hV4Y",
        "colab_type": "text"
      },
      "source": [
        "# **Fixed Text Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPW-etZBhXeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract all fixed text time stamps to retrieve fixed text data\n",
        "annotations_dir = './FreeTextMarkerUnixTimeStamp/'\n",
        "\n",
        "desktop_annotations = pd.read_csv(annotations_dir+'Desktop_Freetext.csv').values\n",
        "phone_annotations = pd.read_csv(annotations_dir+'Phone_Freetext.csv').values\n",
        "tablet_annotations = pd.read_csv(annotations_dir+'Tablet_Freetext.csv').values\n",
        "\n",
        "desktop_calibrated_annotations = {}\n",
        "for annot in desktop_annotations:\n",
        "    desktop_calibrated_annotations[int(annot[0])] = int(annot[1])\n",
        "\n",
        "phone_calibrated_annotations = {}\n",
        "for annot in phone_annotations:\n",
        "    phone_calibrated_annotations[int(annot[0])] = int(annot[1])\n",
        "\n",
        "tablet_calibrated_annotations = {}\n",
        "for annot in tablet_annotations:\n",
        "    tablet_calibrated_annotations[int(annot[0])] = int(annot[1])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x-s_NMWhecM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function that takes in the entire raw keystroke data and a target fixed text timestamp, \n",
        "# and returns all the keystrokes upto that timestamp\n",
        "\n",
        "def return_target_csv(data, target_time):\n",
        "      target_csv = []\n",
        "      for i, data_item in enumerate(data):\n",
        "          if(int(data_item[3])<int(target_time)):\n",
        "              target_csv.append([data_item[0], data_item[1], data_item[2], data_item[3]])\n",
        "      return np.asarray(target_csv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqFVrJlhhlH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract new CSVs for desktop fixed text\n",
        "data_dir = 'Desktop/'\n",
        "target_dir = 'Desktop_fixed_text/'\n",
        "\n",
        "user_files = os.listdir(data_dir)\n",
        "for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(data_dir+user_file)\n",
        "        user_data = data_frame.values\n",
        "        curr_user_ind = int(user_file[user_file.find('r')+1:user_file.find('.')])\n",
        "        try:\n",
        "            target_time_stamp = desktop_calibrated_annotations[curr_user_ind]\n",
        "            csv = return_target_csv(user_data, target_time_stamp)\n",
        "        except KeyError as e:\n",
        "            csv = user_data\n",
        "\n",
        "        f = open(target_dir+'User'+str(curr_user_ind)+'.csv', 'w')\n",
        "        for line in csv:\n",
        "            f.write('\"'+str(line[0])+'\",\"'+str(line[1])+'\",\"'+str(line[2])+'\",\"'+str(line[3])+'\"\\n')\n",
        "        f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGME8o-Fh0LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract new CSVs for phone fixed text\n",
        "data_dir = 'Phone/'\n",
        "target_dir = 'Phone_fixed_text/'\n",
        "\n",
        "user_files = os.listdir(data_dir)\n",
        "for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(data_dir+user_file)\n",
        "        user_data = data_frame.values\n",
        "        curr_user_ind = int(user_file[user_file.find('r')+1:user_file.find('.')])\n",
        "        try:\n",
        "            target_time_stamp = phone_calibrated_annotations[curr_user_ind]\n",
        "            csv = return_target_csv(user_data, target_time_stamp)\n",
        "        except KeyError as e:\n",
        "            csv = user_data\n",
        "\n",
        "        f = open(target_dir+'User'+str(curr_user_ind)+'.csv', 'w')\n",
        "        for line in csv:\n",
        "            f.write('\"'+str(line[0])+'\",\"'+str(line[1])+'\",\"'+str(line[2])+'\",\"'+str(line[3])+'\"\\n')\n",
        "        f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAXVP6fKh8Rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract new CSVs for tablet fixed text\n",
        "data_dir = 'Tablet/'\n",
        "target_dir = 'Tablet_fixed_text/'\n",
        "\n",
        "user_files = os.listdir(data_dir)\n",
        "for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(data_dir+user_file)\n",
        "        user_data = data_frame.values\n",
        "        curr_user_ind = int(user_file[user_file.find('r')+1:user_file.find('.')])\n",
        "        try:\n",
        "            target_time_stamp = tablet_calibrated_annotations[curr_user_ind]\n",
        "            csv = return_target_csv(user_data, target_time_stamp)\n",
        "        except KeyError as e:\n",
        "            csv = user_data\n",
        "\n",
        "        f = open(target_dir+'User'+str(curr_user_ind)+'.csv', 'w')\n",
        "        for line in csv:\n",
        "            f.write('\"'+str(line[0])+'\",\"'+str(line[1])+'\",\"'+str(line[2])+'\",\"'+str(line[3])+'\"\\n')\n",
        "        f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxYKMXUE1970",
        "colab_type": "text"
      },
      "source": [
        "# **KHT feature (Unigraph) extraction utilities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiPEEMUztyV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get KHT feature based on current key and timing values\n",
        "def get_KHT(keys_in_pipeline, search_key, search_key_timing):\n",
        "    mask = np.ones(len(keys_in_pipeline))\n",
        "    keys_in_pipeline = np.asarray(keys_in_pipeline)\n",
        "\n",
        "    for i, (key, timing) in enumerate(keys_in_pipeline):\n",
        "          if(search_key==key):\n",
        "              mask[i] = 0\n",
        "              kht = int(float(search_key_timing))-int(float(timing))\n",
        "              non_zero_indices = np.nonzero(mask) \n",
        "              if(len(non_zero_indices)>0):\n",
        "                  keys_in_pipeline = keys_in_pipeline[non_zero_indices]\n",
        "              else:\n",
        "                  keys_in_pipeline = []\n",
        "              return keys_in_pipeline, kht\n",
        "\n",
        "    return keys_in_pipeline, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K8yFtZIsbvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to get KHT feature dictionary for a given user\n",
        "def get_KHT_features(data):\n",
        "    feature_dictionary = {}\n",
        "    keys_in_pipeline = []\n",
        "\n",
        "    for row_idx in range(len(data)):\n",
        "        keys_in_pipeline = list(keys_in_pipeline)\n",
        "        curr_key = data[row_idx][1]\n",
        "        curr_direction = data[row_idx][2]\n",
        "        curr_timing = data[row_idx][3]\n",
        "\n",
        "        if(curr_direction==0):\n",
        "            keys_in_pipeline.append([curr_key, curr_timing])\n",
        "\n",
        "        if(curr_direction==1):\n",
        "            keys_in_pipeline, curr_kht = get_KHT(keys_in_pipeline, curr_key, curr_timing)\n",
        "            if(curr_kht is None):\n",
        "                  continue\n",
        "            else:\n",
        "                  if(curr_key in list(feature_dictionary.keys())):\n",
        "                        feature_dictionary[curr_key].append(curr_kht)\n",
        "                  else:\n",
        "                        feature_dictionary[curr_key] = []\n",
        "                        feature_dictionary[curr_key].append(curr_kht)\n",
        "\n",
        "    return feature_dictionary         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14fI2AOderC9",
        "colab_type": "text"
      },
      "source": [
        "# **KIT Data (Digraph) Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL0C7lzxeqJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get KIT feature based on current key and timing values\n",
        "def get_timings_KIT(keys_in_pipeline, search_key, search_key_timing):\n",
        "    mask = np.ones(len(keys_in_pipeline))\n",
        "    keys_in_pipeline = np.asarray(keys_in_pipeline)\n",
        "    for i, (key, timing) in enumerate(keys_in_pipeline):\n",
        "          if(search_key==key):\n",
        "              mask[i] = 0\n",
        "              non_zero_indices = np.nonzero(mask) \n",
        "\n",
        "              if(len(non_zero_indices)>0):\n",
        "                  keys_in_pipeline = keys_in_pipeline[non_zero_indices]\n",
        "              else:\n",
        "                  keys_in_pipeline = []\n",
        "\n",
        "              return keys_in_pipeline, timing, search_key_timing\n",
        "    return keys_in_pipeline, None, None\n",
        "\n",
        "# function to get KIT data frame with key, press_time, release_time for a given user\n",
        "def get_dataframe_KIT(data):\n",
        "    \"\"\" Input: data  Output: Dataframe with (key, press_time, release_time)\"\"\" \n",
        "    feature_dictionary = {}\n",
        "    keys_in_pipeline = []\n",
        "    result_key = []\n",
        "    press = []\n",
        "    release = []\n",
        "    for row_idx in range(len(data)):\n",
        "        keys_in_pipeline = list(keys_in_pipeline)\n",
        "        curr_key = data[row_idx][1]\n",
        "        curr_direction = data[row_idx][2]\n",
        "        curr_timing = data[row_idx][3]\n",
        "\n",
        "        if(curr_direction==0):\n",
        "            keys_in_pipeline.append([curr_key, curr_timing])\n",
        "\n",
        "        if(curr_direction==1):\n",
        "            keys_in_pipeline, curr_start, curr_end = get_timings_KIT(keys_in_pipeline, curr_key, curr_timing)\n",
        "            if(curr_start is None):\n",
        "                continue\n",
        "            else:\n",
        "                result_key.append(curr_key)\n",
        "                press.append(curr_start)\n",
        "                release.append(curr_end)\n",
        "\n",
        "    resultant_data_frame = pd.DataFrame(list(zip(result_key, press, release)),\n",
        "               columns =['Key', 'Press_Time', 'Release_Time']) \n",
        "    return resultant_data_frame      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMJi6NzhFLXu",
        "colab_type": "text"
      },
      "source": [
        "# **KIT feature (Digraph) extraction utilities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaqu_8OFFJDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to get Flight1 KIT feature dictionary for a given user\n",
        "def get_KIT_features_F1(data):\n",
        "    \"\"\" Input: keystroke data, Output: Dictionary of (next_key_press - current_key_release) \"\"\"\n",
        "    feature_dictionary = {}\n",
        "\n",
        "    for row_idx in range(0, len(data)):\n",
        "        curr_key = data[row_idx][0]\n",
        "        if(row_idx + 1 >= len(data)):\n",
        "            break\n",
        "        next_key = data[row_idx + 1][0]\n",
        "        curr_timing = data[row_idx][2]\n",
        "        next_timing = data[row_idx+1][1]\n",
        "        \n",
        "        if(str(curr_key)+str(next_key) in list(feature_dictionary.keys())):\n",
        "            feature_dictionary[str(curr_key)+str(next_key)].append(int(float(next_timing))-int(float(curr_timing)))\n",
        "        else:\n",
        "            feature_dictionary[str(curr_key)+str(next_key)] = []\n",
        "            feature_dictionary[str(curr_key)+str(next_key)].append(int(float(next_timing))-int(float(curr_timing)))\n",
        "\n",
        "    return feature_dictionary  \n",
        "\n",
        "# function to get Flight2 KIT feature dictionary for a given user\n",
        "def get_KIT_features_F2(data):\n",
        "    \"\"\" Input: keystroke data, Output: Dictionary of (next_key_press - current_key_press) \"\"\"\n",
        "    feature_dictionary = {}\n",
        "\n",
        "    for row_idx in range(0, len(data)):\n",
        "        curr_key = data[row_idx][0]\n",
        "        if(row_idx + 1 >= len(data)):\n",
        "            break\n",
        "        next_key = data[row_idx + 1][0]\n",
        "        curr_timing = data[row_idx][1]\n",
        "        next_timing = data[row_idx+1][1]\n",
        "        if(str(curr_key)+str(next_key) in list(feature_dictionary.keys())):\n",
        "            feature_dictionary[str(curr_key)+str(next_key)].append(int(float(next_timing))-int(float(curr_timing)))\n",
        "        else:\n",
        "            feature_dictionary[str(curr_key)+str(next_key)] = []\n",
        "            feature_dictionary[str(curr_key)+str(next_key)].append(int(float(next_timing))-int(float(curr_timing)))\n",
        "\n",
        "    return feature_dictionary  \n",
        "\n",
        "# function to get Flight3 KIT feature dictionary for a given user\n",
        "def get_KIT_features_F3(data):\n",
        "    \"\"\" Input: keystroke data, Output: Dictionary of (next_key_release - current_key_release) \"\"\"\n",
        "    feature_dictionary = {}\n",
        "\n",
        "    for row_idx in range(0, len(data)):\n",
        "        curr_key = data[row_idx][0]\n",
        "        if(row_idx + 1 >= len(data)):\n",
        "            break\n",
        "        next_key = data[row_idx + 1][0]\n",
        "        curr_timing = data[row_idx][2]\n",
        "        next_timing = data[row_idx+1][2]\n",
        "        if(str(curr_key)+str(next_key) in list(feature_dictionary.keys())):\n",
        "            feature_dictionary[str(curr_key)+str(next_key)].append(int(float(next_timing))-int(float(curr_timing)))\n",
        "        else:\n",
        "            feature_dictionary[str(curr_key)+str(next_key)] = []\n",
        "            feature_dictionary[str(curr_key)+str(next_key)].append(int(float(next_timing))-int(float(curr_timing)))\n",
        "\n",
        "    return feature_dictionary  \n",
        "\n",
        "# function to get Flight3 KIT feature dictionary for a given user\n",
        "def get_KIT_features_F4(data):\n",
        "    \"\"\" Input: keystroke data, Output: Dictionary of (next_key_release - current_key_press) \"\"\"\n",
        "    feature_dictionary = {}\n",
        "\n",
        "    for row_idx in range(0, len(data)):\n",
        "        curr_key = data[row_idx][0]\n",
        "        if(row_idx + 1 >= len(data)):\n",
        "            break\n",
        "        next_key = data[row_idx + 1][0]\n",
        "        curr_timing = data[row_idx][1]\n",
        "        next_timing = data[row_idx+1][2]\n",
        "        if(str(curr_key)+str(next_key) in list(feature_dictionary.keys())):\n",
        "            feature_dictionary[str(curr_key)+str(next_key)].append(int(float(next_timing))-int(float(curr_timing)))\n",
        "        else:\n",
        "            feature_dictionary[str(curr_key)+str(next_key)] = []\n",
        "            feature_dictionary[str(curr_key)+str(next_key)].append(int(float(next_timing))-int(float(curr_timing)))\n",
        "\n",
        "    return feature_dictionary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMAvxjSEn1JJ",
        "colab_type": "text"
      },
      "source": [
        "# **Word Level feature extraction utilities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7dJ24ftoTO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' function to return word level statistics for each extracted word.\n",
        "These features are as follows:\n",
        "1) Word hold time\n",
        "2) Average, Standard Deviation and Median of all key hold times in the word\n",
        "3) Average, Standard Deviation and Median of all flight 1 features for all digraphs in the word\n",
        "4) Average, Standard Deviation and Median of all flight 2 features for all digraphs in the word\n",
        "5) Average, Standard Deviation and Median of all flight 3 features for all digraphs in the word\n",
        "6) Average, Standard Deviation and Median of all flight 4 features for all digraphs in the word\n",
        "'''\n",
        "def get_advanced_word_level_features(words_in_pipeline):\n",
        "    def get_word_hold(words_in_pipeline):\n",
        "        return int(float(words_in_pipeline[-1][2])) - int(float(words_in_pipeline[0][1]))\n",
        "    \n",
        "    def get_avg_std_median_key_hold(words_in_pipeline):\n",
        "        key_holds = []\n",
        "        for _, press, release in words_in_pipeline:\n",
        "            key_holds.append(int(float(release))-int(float(press)))\n",
        "        return np.mean(key_holds), np.std(key_holds), np.median(key_holds)\n",
        "\n",
        "    def get_avg_std_median_flights(words_in_pipeline):\n",
        "        flights_1 = []\n",
        "        flights_2 = []\n",
        "        flights_3 = []\n",
        "        flights_4 = []\n",
        "        for i in range(len(words_in_pipeline)-1):\n",
        "            k1_r = words_in_pipeline[i][2]\n",
        "            k1_p = words_in_pipeline[i][1]\n",
        "            k2_r = words_in_pipeline[i+1][2]\n",
        "            k2_p = words_in_pipeline[i+1][1]\n",
        "            flights_1.append(int(float(k2_p))-int(float(k1_r)))\n",
        "            flights_2.append(int(float(k2_r))-int(float(k1_r)))\n",
        "            flights_3.append(int(float(k2_p))-int(float(k1_p)))\n",
        "            flights_4.append(int(float(k2_r))-int(float(k1_p)))\n",
        "        return np.mean(flights_1), np.std(flights_1), np.median(flights_1), np.mean(flights_2), np.std(flights_2), np.median(flights_2), np.mean(flights_3), np.std(flights_3), np.median(flights_3), np.mean(flights_4), np.std(flights_4), np.median(flights_4)\n",
        "\n",
        "    wh = get_word_hold(words_in_pipeline)\n",
        "    avg_kh, std_kh, median_kh = get_avg_std_median_key_hold(words_in_pipeline)\n",
        "    avg_flight1, std_flight1, median_flight1, avg_flight2, std_flight2, median_flight2, avg_flight3, std_flight3, median_flight3, avg_flight4, std_flight4, median_flight4 = get_avg_std_median_flights(words_in_pipeline)\n",
        "    return [wh, avg_kh, std_kh, median_kh, avg_flight1, std_flight1, median_flight1, avg_flight2, std_flight2, median_flight2, avg_flight3, std_flight3, median_flight3, avg_flight4, std_flight4, median_flight4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QMyvU3gny4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to get the advanced word level features of every user\n",
        "def get_advanced_word_features(processed_data):\n",
        "    words_in_pipeline = []\n",
        "    feature_dictionary = {}\n",
        "\n",
        "    ignore_keys = ['LCTRL', 'RSHIFT', 'TAB', 'DOWN']\n",
        "    delimiter_keys = ['SPACE', '.', ',', 'RETURN']\n",
        "\n",
        "    for row_idx in range(len(processed_data)):\n",
        "        curr_key = processed_data[row_idx][1]\n",
        "        curr_press = processed_data[row_idx][2]\n",
        "        curr_release = processed_data[row_idx][3]\n",
        "\n",
        "        if(curr_key in ignore_keys):\n",
        "              continue\n",
        "\n",
        "        if(curr_key in delimiter_keys):\n",
        "            if(len(words_in_pipeline)>0):\n",
        "                advanced_word_features = get_advanced_word_level_features(words_in_pipeline)\n",
        "                key_word = ''\n",
        "                for char, _, _ in words_in_pipeline:\n",
        "                    key_word=key_word+str(char)\n",
        "                \n",
        "                if(key_word in list(feature_dictionary.keys())):\n",
        "                    feature_dictionary[key_word].append(advanced_word_features)\n",
        "                else:\n",
        "                    feature_dictionary[key_word] = []\n",
        "                    feature_dictionary[key_word].append(advanced_word_features)\n",
        "            words_in_pipeline = []\n",
        "            continue\n",
        "\n",
        "        if(curr_key=='BACKSPACE'):\n",
        "              words_in_pipeline = words_in_pipeline[:-1]\n",
        "              continue\n",
        "\n",
        "        words_in_pipeline.append([curr_key, curr_press, curr_release])\n",
        "\n",
        "    return feature_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC8k4kPC2FGR",
        "colab_type": "text"
      },
      "source": [
        "# **Feature extraction and serialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUTOofQOzuYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the entire feature dictionary given the modality (Desktop, Phone, Tablet)\n",
        "def get_all_users_features_KHT(directory):\n",
        "    users_feat_dict = {}\n",
        "\n",
        "    user_files = os.listdir(directory)\n",
        "    for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(directory+user_file)\n",
        "        user_data = data_frame.values\n",
        "        user_feat_dict = get_KHT_features(user_data)\n",
        "        users_feat_dict[i+1] = user_feat_dict\n",
        "\n",
        "    return users_feat_dict\n",
        "\n",
        "def get_all_users_features_KIT(directory):\n",
        "    users_feat_dict_f1 = {}\n",
        "    users_feat_dict_f2 = {}\n",
        "    users_feat_dict_f3 = {}\n",
        "    users_feat_dict_f4 = {}\n",
        "    user_files = os.listdir(directory)\n",
        "    for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(directory+user_file)\n",
        "        data_frame = get_dataframe_KIT(data_frame.values)\n",
        "        user_data = data_frame.values\n",
        "        \n",
        "        user_feat_dict_f1 = get_KIT_features_F1(user_data)\n",
        "        users_feat_dict_f1[i+1] = user_feat_dict_f1\n",
        "\n",
        "        user_feat_dict_f2 = get_KIT_features_F2(user_data)\n",
        "        users_feat_dict_f2[i+1] = user_feat_dict_f2\n",
        "\n",
        "        user_feat_dict_f3 = get_KIT_features_F3(user_data)\n",
        "        users_feat_dict_f3[i+1] = user_feat_dict_f3\n",
        "\n",
        "        user_feat_dict_f4 = get_KIT_features_F4(user_data)\n",
        "        users_feat_dict_f4[i+1] = user_feat_dict_f4\n",
        "\n",
        "    return users_feat_dict_f1, users_feat_dict_f2, users_feat_dict_f3, users_feat_dict_f4\n",
        "\n",
        "\n",
        "def get_all_users_features_word(directory):\n",
        "    users_feat_dict = {}\n",
        "\n",
        "    user_files = os.listdir(directory)\n",
        "    for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(directory+user_file)\n",
        "        user_data = data_frame.values\n",
        "        user_feat_dict = get_word_features(user_data)\n",
        "        users_feat_dict[i+1] = user_feat_dict\n",
        "\n",
        "    return users_feat_dict\n",
        "\n",
        "def get_all_users_features_advanced_word(directory):\n",
        "    users_feat_dict = {}\n",
        "\n",
        "    user_files = os.listdir(directory)\n",
        "    for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(directory+user_file)\n",
        "        user_data = data_frame.values\n",
        "        processed_data = get_dataframe_KIT(user_data)\n",
        "        processed_data = np.c_[np.arange(len(processed_data)), processed_data]\n",
        "        processed_data = processed_data[np.argsort(processed_data[:, 2])]\n",
        "        user_feat_dict = get_advanced_word_features(processed_data)\n",
        "        users_feat_dict[i+1] = user_feat_dict\n",
        "\n",
        "    return users_feat_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_0laaxA6KRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the entire feature dictionary given the modality for fixed text (Desktop, Phone, Tablet)\n",
        "def get_all_users_features_KHT_fixed(directory):\n",
        "    users_feat_dict = {}\n",
        "\n",
        "    user_files = os.listdir(directory)\n",
        "    for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(directory+user_file)\n",
        "        user_data = data_frame.values\n",
        "        user_feat_dict = get_KHT_features(user_data)\n",
        "        users_feat_dict[i+1] = user_feat_dict\n",
        "\n",
        "    return users_feat_dict\n",
        "\n",
        "def get_all_users_features_KIT_fixed(directory):\n",
        "    users_feat_dict_f1 = {}\n",
        "    users_feat_dict_f2 = {}\n",
        "    users_feat_dict_f3 = {}\n",
        "    users_feat_dict_f4 = {}\n",
        "    user_files = os.listdir(directory)\n",
        "    for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(directory+user_file)\n",
        "        data_frame = get_dataframe_KIT(data_frame.values)\n",
        "        user_data = data_frame.values\n",
        "        \n",
        "        user_feat_dict_f1 = get_KIT_features_F1(user_data)\n",
        "        users_feat_dict_f1[i+1] = user_feat_dict_f1\n",
        "\n",
        "        user_feat_dict_f2 = get_KIT_features_F2(user_data)\n",
        "        users_feat_dict_f2[i+1] = user_feat_dict_f2\n",
        "\n",
        "        user_feat_dict_f3 = get_KIT_features_F3(user_data)\n",
        "        users_feat_dict_f3[i+1] = user_feat_dict_f3\n",
        "\n",
        "        user_feat_dict_f4 = get_KIT_features_F4(user_data)\n",
        "        users_feat_dict_f4[i+1] = user_feat_dict_f4\n",
        "\n",
        "    return users_feat_dict_f1, users_feat_dict_f2, users_feat_dict_f3, users_feat_dict_f4\n",
        "\n",
        "\n",
        "def get_all_users_features_word_fixed(directory):\n",
        "    users_feat_dict = {}\n",
        "\n",
        "    user_files = os.listdir(directory)\n",
        "    for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(directory+user_file)\n",
        "        user_data = data_frame.values\n",
        "        user_feat_dict = get_word_features(user_data)\n",
        "        users_feat_dict[i+1] = user_feat_dict\n",
        "\n",
        "    return users_feat_dict\n",
        "\n",
        "def get_all_users_features_advanced_word_fixed(directory):\n",
        "    users_feat_dict = {}\n",
        "\n",
        "    user_files = os.listdir(directory)\n",
        "    for i in tqdm(range(len(user_files))):\n",
        "        user_file = user_files[i]\n",
        "        data_frame = pd.read_csv(directory+user_file)\n",
        "        user_data = data_frame.values\n",
        "        processed_data = get_dataframe_KIT(user_data)\n",
        "        processed_data = np.c_[np.arange(len(processed_data)), processed_data]\n",
        "        processed_data = processed_data[np.argsort(processed_data[:, 2])]\n",
        "        user_feat_dict = get_advanced_word_features(processed_data)\n",
        "        users_feat_dict[i+1] = user_feat_dict\n",
        "\n",
        "    return users_feat_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayP8f9Fg3wWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KHT feature extraction for Desktop\n",
        "desktop_kht_features = get_all_users_features_KHT('Desktop/')\n",
        "\n",
        "pickle.dump(desktop_kht_features, open('desktop_kht_feature_dictionary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb3mEuWb4gPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KHT fixed text feature extraction for Desktop\n",
        "desktop_kht_features_fixed = get_all_users_features_KHT_fixed('Desktop_fixed_text/')\n",
        "\n",
        "pickle.dump(desktop_kht_features_fixed, open('desktop_kht_feature_dictionary_fixed.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXrSMfaw4iRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KHT feature extraction for Phone\n",
        "phone_kht_features = get_all_users_features_KHT('Phone/')\n",
        "\n",
        "pickle.dump(phone_kht_features, open('phone_kht_feature_dictionary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_vsuh7dbC8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KHT fixed text feature extraction for Phone\n",
        "phone_kht_features_fixed = get_all_users_features_KHT_fixed('Phone_fixed_text/')\n",
        "\n",
        "pickle.dump(phone_kht_features_fixed, open('phone_kht_feature_dictionary_fixed.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi7KQYsA4jSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KHT feature extraction for Tablet\n",
        "tablet_kht_features = get_all_users_features_KHT('Tablet/')\n",
        "\n",
        "pickle.dump(tablet_kht_features, open('tablet_kht_feature_dictionary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aox3Z-jIbUwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KHT fixed text feature extraction for Tablet\n",
        "tablet_kht_features_fixed = get_all_users_features_KHT_fixed('Tablet_fixed_text/')\n",
        "\n",
        "pickle.dump(tablet_kht_features_fixed, open('tablet_kht_feature_dictionary_fixed.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt4F0nrxX09B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KIT feature extraction for Desktop\n",
        "desktop_kit_features_f1, desktop_kit_features_f2, desktop_kit_features_f3, desktop_kit_features_f4 = get_all_users_features_KIT('Desktop/')\n",
        "\n",
        "pickle.dump(desktop_kit_features_f1, open('desktop_kit_feature_f1_dictionary.pickle', 'wb'))\n",
        "pickle.dump(desktop_kit_features_f2, open('desktop_kit_feature_f2_dictionary.pickle', 'wb'))\n",
        "pickle.dump(desktop_kit_features_f3, open('desktop_kit_feature_f3_dictionary.pickle', 'wb'))\n",
        "pickle.dump(desktop_kit_features_f4, open('desktop_kit_feature_f4_dictionary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ljky30VcAIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KIT fixed text feature extraction for Desktop\n",
        "desktop_kit_features_f1_fixed, desktop_kit_features_f2_fixed, desktop_kit_features_f3_fixed, desktop_kit_features_f4_fixed = get_all_users_features_KIT_fixed('Desktop_fixed_text/')\n",
        "\n",
        "pickle.dump(desktop_kit_features_f1_fixed, open('desktop_kit_feature_f1_dictionary_fixed.pickle', 'wb'))\n",
        "pickle.dump(desktop_kit_features_f2_fixed, open('desktop_kit_feature_f2_dictionary_fixed.pickle', 'wb'))\n",
        "pickle.dump(desktop_kit_features_f3_fixed, open('desktop_kit_feature_f3_dictionary_fixed.pickle', 'wb'))\n",
        "pickle.dump(desktop_kit_features_f4_fixed, open('desktop_kit_feature_f4_dictionary_fixed.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6BPsabsD272",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KIT feature extraction for Phone\n",
        "phone_kit_features_f1, phone_kit_features_f2, phone_kit_features_f3, phone_kit_features_f4 = get_all_users_features_KIT('Phone/')\n",
        "\n",
        "pickle.dump(phone_kit_features_f1, open('phone_kit_feature_f1_dictionary.pickle', 'wb'))\n",
        "pickle.dump(phone_kit_features_f2, open('phone_kit_feature_f2_dictionary.pickle', 'wb'))\n",
        "pickle.dump(phone_kit_features_f3, open('phone_kit_feature_f3_dictionary.pickle', 'wb'))\n",
        "pickle.dump(phone_kit_features_f4, open('phone_kit_feature_f4_dictionary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD-qroRDenjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KIT fixed text feature extraction for Phone\n",
        "phone_kit_features_f1_fixed, phone_kit_features_f2_fixed, phone_kit_features_f3_fixed, phone_kit_features_f4_fixed = get_all_users_features_KIT_fixed('Phone_fixed_text/')\n",
        "\n",
        "pickle.dump(phone_kit_features_f1_fixed, open('phone_kit_feature_f1_dictionary_fixed.pickle', 'wb'))\n",
        "pickle.dump(phone_kit_features_f2_fixed, open('phone_kit_feature_f2_dictionary_fixed.pickle', 'wb'))\n",
        "pickle.dump(phone_kit_features_f3_fixed, open('phone_kit_feature_f3_dictionary_fixed.pickle', 'wb'))\n",
        "pickle.dump(phone_kit_features_f4_fixed, open('phone_kit_feature_f4_dictionary_fixed.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPZaIQCyEE9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KIT feature extraction for Tablet\n",
        "tablet_kit_features_f1, tablet_kit_features_f2, tablet_kit_features_f3, tablet_kit_features_f4 = get_all_users_features_KIT('Tablet/')\n",
        "\n",
        "pickle.dump(tablet_kit_features_f1, open('tablet_kit_feature_f1_dictionary.pickle', 'wb'))\n",
        "pickle.dump(tablet_kit_features_f2, open('tablet_kit_feature_f2_dictionary.pickle', 'wb'))\n",
        "pickle.dump(tablet_kit_features_f3, open('tablet_kit_feature_f3_dictionary.pickle', 'wb'))\n",
        "pickle.dump(tablet_kit_features_f4, open('tablet_kit_feature_f4_dictionary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNJ8BR-xfFFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KIT fixed text feature extraction for Tablet\n",
        "tablet_kit_features_f1_fixed, tablet_kit_features_f2_fixed, tablet_kit_features_f3_fixed, tablet_kit_features_f4_fixed = get_all_users_features_KIT_fixed('Tablet_fixed_text/')\n",
        "\n",
        "pickle.dump(tablet_kit_features_f1_fixed, open('tablet_kit_feature_f1_dictionary_fixed.pickle', 'wb'))\n",
        "pickle.dump(tablet_kit_features_f2_fixed, open('tablet_kit_feature_f2_dictionary_fixed.pickle', 'wb'))\n",
        "pickle.dump(tablet_kit_features_f3_fixed, open('tablet_kit_feature_f3_dictionary_fixed.pickle', 'wb'))\n",
        "pickle.dump(tablet_kit_features_f4_fixed, open('tablet_kit_feature_f4_dictionary_fixed.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Heyra2sdY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# advanced word feature extraction for Desktop\n",
        "desktop_advanced_word_features = get_all_users_features_advanced_word('Desktop/')\n",
        "\n",
        "pickle.dump(desktop_advanced_word_features, open('desktop_advanced_word_feature_dictionary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "394Mcj5tgf0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# advanced word fixed text feature extraction for Desktop\n",
        "desktop_advanced_word_features_fixed = get_all_users_features_advanced_word_fixed('Desktop_fixed_text/')\n",
        "\n",
        "pickle.dump(desktop_advanced_word_features_fixed, open('desktop_advanced_word_feature_dictionary_fixed.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pumqzYTgsn05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# advanced word feature extraction for Phone\n",
        "phone_advanced_word_features = get_all_users_features_advanced_word('Phone/')\n",
        "\n",
        "pickle.dump(phone_advanced_word_features, open('phone_advanced_word_feature_dictionary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_l8xbajgurq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# advanced word fixed text feature extraction for Phone\n",
        "phone_advanced_word_features_fixed = get_all_users_features_advanced_word_fixed('Phone_fixed_text/')\n",
        "\n",
        "pickle.dump(phone_advanced_word_features_fixed, open('phone_advanced_word_feature_dictionary_fixed.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE1W5iwUs4yK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# advanced word feature extraction for Tablet\n",
        "tablet_advanced_word_features = get_all_users_features_advanced_word('Tablet/')\n",
        "\n",
        "pickle.dump(tablet_advanced_word_features, open('tablet_advanced_word_feature_dictionary.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-oNRxKng92P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# advanced word fixed text feature extraction for Tablet\n",
        "tablet_advanced_word_features_fixed = get_all_users_features_advanced_word_fixed('Tablet_fixed_text/')\n",
        "\n",
        "pickle.dump(tablet_advanced_word_features_fixed, open('tablet_advanced_word_feature_dictionary_fixed.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k_z0YP1QizW",
        "colab_type": "text"
      },
      "source": [
        "# **Outlier Removal Utility**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEBZIr4nCVtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove outlier points in the distribution using the 1.5IQR rule\n",
        "def remove_outliers(x):\n",
        "    a = np.asarray(x)\n",
        "    upper_quartile = np.percentile(a, 75)\n",
        "    lower_quartile = np.percentile(a, 25)\n",
        "    IQR = (upper_quartile - lower_quartile) * 1.5\n",
        "    quartileSet = (lower_quartile - IQR, upper_quartile + IQR)\n",
        "    result = []\n",
        "    for y in a.tolist():\n",
        "        if y >= quartileSet[0] and y <= quartileSet[1]:\n",
        "            result.append(y)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Cs_t1clAd6",
        "colab_type": "text"
      },
      "source": [
        "# **Feature processing for Classification/Regression tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx-BoayElHF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(features):\n",
        "    \"\"\" Input: All feature dictionary Output: Feature matrix with unique columns\"\"\"\n",
        "    feature_set = []\n",
        "    for key1 in features:\n",
        "        for key2 in features[key1]:\n",
        "            feature_set.append(key2)\n",
        "\n",
        "    # Getting unique columns by removing repeated keys\n",
        "    unique_feature_set = set(feature_set)\n",
        "    unique_feature_set = list(unique_feature_set)\n",
        "\n",
        "    size = len(unique_feature_set)\n",
        "    rows, cols = (len(features), len(unique_feature_set))\n",
        "    feature_vector = [[0 for x in range(len(cols))] for x in range(rows)]\n",
        "\n",
        "    # Updating feature matrix based on present features in dictionary\n",
        "    for key1 in tqdm(features):\n",
        "        for key2 in features[key1]:\n",
        "            for j in range(len(unique_feature_set)):\n",
        "                if unique_feature_set[j] == key2:\n",
        "                    temp = abs(np.median(features[key1][key2]))\n",
        "                    feature_vector[(key1)-1][j] = int(temp)\n",
        "                    break\n",
        "                else:\n",
        "                    feature_vector[(key1)-1][j] = 0\n",
        "    return feature_vector\n",
        "\n",
        "top_feature_KIT_Desktop_F1 = [\"in\",\"nSPACE\", \"et\", \"lSPACE\", \"oSPACE\", \"ca\", \"iSPACE\", \"pl\", \"ve\", \"ha\", \"ne\", \"da\", \"he\", \"wi\"]\n",
        "top_feature_KIT_Tablet_F1 = [\"me\", \"is\", \"ne\", \"ha\"]\n",
        "top_feature_KIT_Tablet_F2 = [\"ha\", \"is\", \"me\"]\n",
        "top_feature_KIT_Tablet_F3 = [\"ne\"]\n",
        "top_feature_KIT_Phone_F1 = [\"BACKSPACEBACKSPACE\"]\n",
        "top_feature_KIT_Phone_F2 = [\"BACKSPACEBACKSPACE\"]\n",
        "top_feature_KIT_Phone_F4 = [\"BACKSPACEBACKSPACE\"]\n",
        "\n",
        "def top_feature_KIT(pickle_file, top_feature):\n",
        "    kit_feature_dictionary = pickle.load(open(pickle_file, 'rb'))\n",
        "    selected_top_feature = [[0 for x in range(len(top_feature))] for x in range(116)] \n",
        "    for key1 in kit_feature_dictionary:\n",
        "        if key1 == 117:\n",
        "            break\n",
        "        for i in range(len(top_feature)):\n",
        "            for key2 in kit_feature_dictionary[key1]:\n",
        "                if (str(top_feature[i]) == str(key2)):\n",
        "                    selected_top_feature[key1-1][i] = np.median(kit_feature_dictionary[key1][key2])\n",
        "                    break\n",
        "    return selected_top_feature\n",
        "\n",
        "def concatenated_feature_matrix_KIT():\n",
        "    feature_KIT_Desktop_F1 = top_feature_KIT(\"desktop_kit_feature_f1_dictionary.pickle\", top_feature_KIT_Desktop_F1)\n",
        "    feature_KIT_Tablet_F1 = top_feature_KIT(\"tablet_kit_feature_f1_dictionary.pickle\", top_feature_KIT_Tablet_F1)\n",
        "    feature_KIT_Tablet_F2 = top_feature_KIT(\"tablet_kit_feature_f2_dictionary.pickle\", top_feature_KIT_Tablet_F2)\n",
        "    feature_KIT_Tablet_F3 = top_feature_KIT(\"tablet_kit_feature_f3_dictionary.pickle\", top_feature_KIT_Tablet_F3)\n",
        "    feature_KIT_Phone_F1 = top_feature_KIT(\"phone_kit_feature_f1_dictionary.pickle\", top_feature_KIT_Phone_F1)\n",
        "    feature_KIT_Phone_F2 = top_feature_KIT(\"phone_kit_feature_f2_dictionary.pickle\", top_feature_KIT_Phone_F2)\n",
        "    feature_KIT_Phone_F4 = top_feature_KIT(\"phone_kit_feature_f4_dictionary.pickle\", top_feature_KIT_Phone_F4)\n",
        "    return np.concatenate((np.array(feature_KIT_Desktop_F1), np.array(feature_KIT_Tablet_F1), np.array(feature_KIT_Tablet_F2), np.array(feature_KIT_Tablet_F3), np.array(feature_KIT_Phone_F1), np.array(feature_KIT_Phone_F2), np.array(feature_KIT_Phone_F4)), axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF0MgiFb4r0x",
        "colab_type": "text"
      },
      "source": [
        "## **KHT (Unigraph) Features (Desktop Only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQP4vc-r4zRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Desktop Only KHT\n",
        "0.02974 ./KDE_plots/Desktop/Gender/KHT/kht_v.png v\n",
        "0.0202 ./KDE_plots/Desktop/Gender/KHT/kht_n.png n\n",
        "0.01732 ./KDE_plots/Desktop/Gender/KHT/kht_s.png s\n",
        "0.01723 ./KDE_plots/Desktop/Gender/KHT/kht_h.png h\n",
        "0.01613 ./KDE_plots/Desktop/Gender/KHT/kht_BACKSPACE.png BACKSPACE\n",
        "0.01502 ./KDE_plots/Desktop/Gender/KHT/kht_r.png r\n",
        "0.01446 ./KDE_plots/Desktop/Gender/KHT/kht_u.png u\n",
        "0.01405 ./KDE_plots/Desktop/Gender/KHT/kht_m.png m\n",
        "0.01368 ./KDE_plots/Desktop/Gender/KHT/kht_i.png i\n",
        "0.0134 ./KDE_plots/Desktop/Gender/KHT/kht_p.png p\n",
        "0.01212 ./KDE_plots/Desktop/Gender/KHT/kht_t.png t\n",
        "0.01141 ./KDE_plots/Desktop/Gender/KHT/kht_d.png d\n",
        "0.00996 ./KDE_plots/Desktop/Gender/KHT/kht_o.png o\n",
        "0.00845 ./KDE_plots/Desktop/Gender/KHT/kht_SPACE.png SPACE\n",
        "0.00775 ./KDE_plots/Desktop/Gender/KHT/kht_l.png l\n",
        "0.00595 ./KDE_plots/Desktop/Gender/KHT/kht_q.png q\n",
        "0.00324 ./KDE_plots/Desktop/Gender/KHT/kht_e.png e\n",
        "0.00307 ./KDE_plots/Desktop/Gender/KHT/kht_..png .\n",
        "0.00234 ./KDE_plots/Desktop/Gender/KHT/kht_c.png c\n",
        "0.00215 ./KDE_plots/Desktop/Gender/KHT/kht_w.png w\n",
        "0.00175 ./KDE_plots/Desktop/Gender/KHT/kht_y.png y\n",
        "0.00056 ./KDE_plots/Desktop/Gender/KHT/kht_f.png f\n",
        "0.00052 ./KDE_plots/Desktop/Gender/KHT/kht_a.png a'''\n",
        "\n",
        "feature_list_Desktop_KHT = [\"v\", \"n\", \"s\", \"h\", \"BACKSPACE\", \"r\", \"u\", \"m\", \"i\",\"p\",\"t\",\"d\", \"o\", \"SPACE\", \"l\", \"q\", \"e\", \".\", \"c\", \"w\", \"f\", \"a\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaIDDgrF50hl",
        "colab_type": "text"
      },
      "source": [
        "## **KHT (Unigraph) Features (Phone Only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_bJWWOp55vD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Phone only KHT\n",
        "0.01494 ./KDE_plots/Phone/Gender/KHT/kht_q.png q\n",
        "0.01181 ./KDE_plots/Phone/Gender/KHT/kht_BACKSPACE.png BACKSPACE\n",
        "0.01177 ./KDE_plots/Phone/Gender/KHT/kht_w.png w\n",
        "0.00993 ./KDE_plots/Phone/Gender/KHT/kht_..png .\n",
        "0.00887 ./KDE_plots/Phone/Gender/KHT/kht_s.png s\n",
        "0.00668 ./KDE_plots/Phone/Gender/KHT/kht_l.png l\n",
        "0.00568 ./KDE_plots/Phone/Gender/KHT/kht_g.png g\n",
        "0.00526 ./KDE_plots/Phone/Gender/KHT/kht_a.png a\n",
        "0.00451 ./KDE_plots/Phone/Gender/KHT/kht_y.png y\n",
        "0.00388 ./KDE_plots/Phone/Gender/KHT/kht_e.png e\n",
        "0.00294 ./KDE_plots/Phone/Gender/KHT/kht_SPACE.png SPACE\n",
        "0.00266 ./KDE_plots/Phone/Gender/KHT/kht_i.png i\n",
        "0.00261 ./KDE_plots/Phone/Gender/KHT/kht_o.png o\n",
        "0.00257 ./KDE_plots/Phone/Gender/KHT/kht_m.png m\n",
        "0.00239 ./KDE_plots/Phone/Gender/KHT/kht_p.png p\n",
        "0.00217 ./KDE_plots/Phone/Gender/KHT/kht_r.png r\n",
        "0.00203 ./KDE_plots/Phone/Gender/KHT/kht_d.png d\n",
        "0.00139 ./KDE_plots/Phone/Gender/KHT/kht_n.png n\n",
        "0.00133 ./KDE_plots/Phone/Gender/KHT/kht_h.png h\n",
        "0.00124 ./KDE_plots/Phone/Gender/KHT/kht_u.png u\n",
        "0.00113 ./KDE_plots/Phone/Gender/KHT/kht_v.png v\n",
        "0.00092 ./KDE_plots/Phone/Gender/KHT/kht_c.png c\n",
        "0.00081 ./KDE_plots/Phone/Gender/KHT/kht_t.png t\n",
        "0.00065 ./KDE_plots/Phone/Gender/KHT/kht_f.png f'''\n",
        "\n",
        "feature_list_Phone_KHT = [\"q\", \"BACKSPACE\", \"w\", \".\", \"s\", \"l\", \"g\", \"a\", \"y\", \"e\", \"SPACE\", \"i\", \"o\", \"m\", \"p\", \"r\", \"d\", \"n\", \"h\", \"u\", \"v\", \"c\", \"t\", \"f\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG3Jgn5J6uyI",
        "colab_type": "text"
      },
      "source": [
        "## **KHT (Unigraph) Features (Tablet Only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRwRx6xt6zTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Tablet Only KHT\n",
        "0.01702 ./KDE_plots/Tablet/Gender/KHT/kht_BACKSPACE.png BACKSPACE\n",
        "0.0148 ./KDE_plots/Tablet/Gender/KHT/kht_r.png r\n",
        "0.01267 ./KDE_plots/Tablet/Gender/KHT/kht_v.png v\n",
        "0.01234 ./KDE_plots/Tablet/Gender/KHT/kht_p.png p\n",
        "0.01105 ./KDE_plots/Tablet/Gender/KHT/kht_c.png c\n",
        "0.00729 ./KDE_plots/Tablet/Gender/KHT/kht_e.png e\n",
        "0.00532 ./KDE_plots/Tablet/Gender/KHT/kht_..png .\n",
        "0.00407 ./KDE_plots/Tablet/Gender/KHT/kht_f.png f\n",
        "0.00364 ./KDE_plots/Tablet/Gender/KHT/kht_a.png a\n",
        "0.00274 ./KDE_plots/Tablet/Gender/KHT/kht_SPACE.png SPACE\n",
        "0.0023 ./KDE_plots/Tablet/Gender/KHT/kht_d.png d\n",
        "0.00227 ./KDE_plots/Tablet/Gender/KHT/kht_h.png h\n",
        "0.00219 ./KDE_plots/Tablet/Gender/KHT/kht_l.png l\n",
        "0.00218 ./KDE_plots/Tablet/Gender/KHT/kht_b.png b\n",
        "0.00202 ./KDE_plots/Tablet/Gender/KHT/kht_u.png u\n",
        "0.00181 ./KDE_plots/Tablet/Gender/KHT/kht_t.png t\n",
        "0.0015 ./KDE_plots/Tablet/Gender/KHT/kht_o.png o\n",
        "0.00146 ./KDE_plots/Tablet/Gender/KHT/kht_m.png m\n",
        "0.00139 ./KDE_plots/Tablet/Gender/KHT/kht_y.png y\n",
        "0.00132 ./KDE_plots/Tablet/Gender/KHT/kht_i.png i\n",
        "0.0012 ./KDE_plots/Tablet/Gender/KHT/kht_n.png n\n",
        "0.00093 ./KDE_plots/Tablet/Gender/KHT/kht_s.png s\n",
        "0.00064 ./KDE_plots/Tablet/Gender/KHT/kht_w.png w'''\n",
        "\n",
        "feature_list_Tablet_KHT = [\"BACKSPACE\", \"r\", \"v\", \"p\", \"c\", \"e\", \".\", \"f\", \"a\", \"SPACE\", \"d\", \"h\", \"l\", \"b\", \"u\", \"t\", \"o\", \"m\", \"y\", \"i\", \"n\", \"s\", \"w\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk1OolCqjPwe",
        "colab_type": "text"
      },
      "source": [
        "## **KIT (Digraph) Features (Desktop Only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm-eQrQMjN6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_list_Desktop_KIT_1 = ['ir', 'ot', 've', 'no', 'ha', 'he', 'to', 'rd', 'hSPACE', 'is', 'co', 'un', 'pSPACE', 'di', 'fi', 'ni', 'pl', 'ne', 'fSPACE', 'hi', 'tSPACE', 'es', 'eSPACE', 'on', 'dSPACE', 'oSPACE', 'rl', 'me', 'le', 'wi', 'if', 'nc', 'lSPACE', 'SPACEs', 'SPACEa', 'nt', 'pe', 'or', 'iSPACE', 'wo', 'SPACEt', 'ca', 'nd', 'ce', 'ly', 'ov', 'te', 'ed', 'th', 'tw', 'ff', 'in', 'aSPACE', 'SPACEl', 'li', 'se', 'it', 'la', 'SPACEn', 'of', 'ul', 'ec', 'ct', 'yp', 'et', 'SPACEc', 'io', 'sa', 'ts', 're', 'SPACEf', 'ti', 'il', 'ap', 'ue', 'ySPACE', 'ds', 'SPACEh', 'el', 'da', 'SPACEo', 'ol', 'll', 'ef', 'SPACEp', 'ta', 'st', 'am', 'SPACEw', 'ar', 'rs', 'en', 'er', 'sSPACE', 'qu', 's.', 'at', '.SPACE', 'nSPACE', 'SPACEu', 'SPACEm', 'as', 'SPACEi', 'e.', 'SPACEd', 'si', 'ee', 'ss']\n",
        "\n",
        "feature_list_Desktop_KIT_2 = ['rd', 'rl', 'to', 'un', 'ap', 'ot', 'el', 'ol', 'ir', 'ef', 'rs', 'ov', 'yp', 'ly', 'nSPACE', 'SPACEm', 'ue', 'if', 'no', 'nt', 've', 'ed', 'SPACEs', 'iSPACE', 'es', 'co', 'ee', 'SPACEn', 'wi', 'st', 'ni', 'it', 'ct', 'nc', 'et', 'hi', 'ce', 'pSPACE', 'oSPACE', 'hSPACE', 'SPACEf', 'pe', 'sa', 'se', 're', 'of', 'ts', 'SPACEu', 'di', 'la', 'ff', 'si', 'aSPACE', 'ta', 'ti', 'en', 'ySPACE', 'pl', 'ss', 'da', 'fSPACE', 'li', 'tw', 'lSPACE', 'th', 'SPACEc', 'il', 'ne', 'is', 'te', 'or', 'le', 'ul', 'SPACEp', 'as', 'SPACEl', 's.', '.SPACE', 'in', 'me', 'ds', 'dSPACE', 'SPACEa', 'er', 'nd', 'SPACEt', 'fi', 'he', 'ha', 'qu', 'll', 'ec', 'am', 'SPACEd', 'on', 'ca', 'at', 'sSPACE', 'SPACEi', 'SPACEo', 'wo', 'tSPACE', 'SPACEh', 'io', 'eSPACE', 'e.', 'SPACEw', 'ar']\n",
        "\n",
        "feature_list_Desktop_KIT_3 = ['un', 'to', 'li', 'rl', 'es', 'nt', 'ce', 'hSPACE', 'th', 'iSPACE', 'rd', 'fi', 'fSPACE', 'el', 'pl', 'si', 'ef', 'en', 'ff', 'SPACEs', 'oSPACE', 'ed', 'co', 'ov', 'ap', 'ni', 'if', 'dSPACE', 'il', 'll', 'SPACEn', 'sa', 'ds', 'ot', 'as', 'ir', 'et', 'pSPACE', 'SPACEc', 'tw', 'ca', 've', 'he', 'it', 'lSPACE', 'SPACEt', 'ar', 'SPACEf', 'in', 'ue', 'am', 'ct', 'la', 'no', 'wi', 'nSPACE', 'ti', 'ne', 'ec', 'SPACEp', 'ta', 'is', 'st', 'rs', 'pe', 'ss', 'or', 're', 'hi', 'io', 'ts', 'SPACEw', 'se', 'aSPACE', 'ySPACE', 'ha', 'nc', 'ol', 'da', 'of', 'SPACEu', 'le', 'ly', 'SPACEm', 'SPACEl', 'on', 'at', 's.', 'sSPACE', 'yp', 'di', 'SPACEa', 'te', 'ee', 'SPACEd', 'eSPACE', 'er', 'SPACEi', 'ul', '.SPACE', 'me', 'qu', 'e.', 'SPACEo', 'wo', 'SPACEh', 'nd', 'tSPACE']\n",
        "\n",
        "feature_list_Desktop_KIT_4 = ['un', 'pl', 'rl', 'rd', 'ol', 'in', 'as', 'ds', 'en', 'me', 'ef', 'to', 'iSPACE', 'll', 'ir', 'nt', 'fSPACE', 'ot', 'at', 'ar', 'ee', 'if', 'am', 'fi', 'el', 'SPACEm', 'co', 'ed', 'ap', 'SPACEu', 'di', 'aSPACE', 'hSPACE', 'ly', 'hi', 'ov', 'it', 'SPACEn', 'ct', 'SPACEi', 'ff', 'ca', 'tw', 'ce', 'SPACEf', 'es', 'wo', 'ni', 'si', 'eSPACE', 'et', 'ti', 'th', 'nc', 'ySPACE', 'da', 'SPACEs', 'qu', 'pe', 'er', 'SPACEl', 'or', 'oSPACE', 'nSPACE', 'li', 'SPACEc', 'ul', 'pSPACE', 'st', 'of', 'il', 'wi', 'on', 'SPACEp', 'la', 'yp', 's.', 'SPACEt', 'sSPACE', 'rs', 'dSPACE', 'he', 'sa', 'ec', 'te', 'lSPACE', 'ta', 'ha', 'SPACEw', 'nd', 'ss', 'ue', 'ts', 'no', 'ne', 've', 'io', 'is', 'SPACEd', '.SPACE', 'SPACEa', 'e.', 'SPACEo', 'se', 'SPACEh', 'tSPACE', 'le', 're']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4nVMSiWlF_H",
        "colab_type": "text"
      },
      "source": [
        "## **KIT (Digraph) Features (Phone Only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-5HDWcYlJw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_list_Phone_KIT_1 = ['am', 'wo', 'pl', 'me', 'fu', 'le', 'en', 'dSPACE', 'is', 'ha', 'ti', 'ce', 'er', 'to', 'of', 'nd', 'st', 'lSPACE', 'SPACEo', 'nt', 've', 'ct', 'nc', 'se', 'BACKSPACEBACKSPACE', 'aSPACE', 'at', 'co', 'ff', 'ed', 'or', 'ni', 'te', 'ta', 'io', 'SPACEf', 'wi', 'da', 'qu', 'li', 'sSPACE', 'eSPACE', 'oSPACE', 'ol', 'rd', 'ee', 'th', 'nSPACE', 'if', 'in', 'he', 'SPACEw', 'ySPACE', 'hi', 'fSPACE', 'as', 'll', 'SPACEu', 'es', 'on', 'ec', 'iq', 'ar', 'SPACEt', 'e.', 'SPACEa', 'SPACEi', 're', 'SPACEd', 'tSPACE', 'SPACEs']\n",
        "\n",
        "feature_list_Phone_KIT_2 = ['BACKSPACEBACKSPACE', 'am', 'wo', 'me', 'le', 'rd', 'te', 'pl', 'SPACEo', 'fu', 'er', 'ce', 'lSPACE', 'dSPACE', 'st', 'is', 'of', 'da', 'to', 'ta', 'ct', 've', 'se', 'nd', 'ti', 'at', 'qu', 'ec', 'en', 'wi', 'if', 'fSPACE', 'nc', 'iq', 'or', 'eSPACE', 'ed', 'th', 'as', 'nt', 'nSPACE', 'sSPACE', 'ha', 'aSPACE', 'SPACEw', 'SPACEu', 'ff', 'co', 'ni', 'io', 'hi', 'he', 'li', 'ol', 'es', 'ee', 'ySPACE', 'SPACEf', 'oSPACE', 'e.', 'll', 'SPACEt', 'in', 'SPACEa', 'on', 're', 'SPACEd', 'tSPACE', 'SPACEs', 'ar', 'SPACEi']\n",
        "\n",
        "feature_list_Phone_KIT_3 = ['wo', 'am', 'le', 'se', 'fu', 've', 'to', 'dSPACE', 'SPACEo', 'st', 'qu', 'en', 'me', 'ni', 'io', 'ol', 'te', 'er', 'ti', 'da', 'rd', 'wi', 'at', 'ce', 'co', 'ed', 'aSPACE', 'of', 'iq', 'ta', 'pl', 'eSPACE', 'sSPACE', 'ct', 'fSPACE', 'nd', 'lSPACE', 'th', 'nt', 'ee', 'is', 'es', 'hi', 'nSPACE', 'li', 'nc', 'SPACEw', 'SPACEf', 'SPACEu', 'or', 're', 'ec', 'ha', 'on', 'as', 'ySPACE', 'oSPACE', 'tSPACE', 'ar', 'ff', 'if', 'e.', 'in', 'SPACEt', 'BACKSPACEBACKSPACE', 'SPACEa', 'll', 'he', 'SPACEd', 'SPACEi', 'SPACEs']\n",
        "\n",
        "feature_list_Phone_KIT_4 = ['wo', 'am', 'le', 'te', 'dSPACE', 'of', 've', 'ta', 'st', 'ff', 'io', 'se', 'ed', 'SPACEo', 'me', 'ce', 'to', 'er', 'th', 'fu', 'da', 'ee', 'rd', 'iq', 'if', 'as', 'wi', 'qu', 'lSPACE', 'nd', 'ni', 'fSPACE', 'aSPACE', 'ec', 'en', 'ti', 'at', 'co', 'nc', 'eSPACE', 'es', 're', 'll', 'BACKSPACEBACKSPACE', 'nSPACE', 'sSPACE', 'ct', 'nt', 'hi', 'SPACEu', 'ol', 'SPACEw', 'pl', 'in', 'is', 'or', 'li', 'ha', 'e.', 'he', 'ySPACE', 'SPACEf', 'on', 'oSPACE', 'SPACEi', 'tSPACE', 'SPACEd', 'SPACEs', 'SPACEa', 'SPACEt', 'ar']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ17MmSVmKf7",
        "colab_type": "text"
      },
      "source": [
        "## **KIT (Digraph) Features (Tablet Only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6nvCrmgmONN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_list_Tablet_KIT_1 = ['ne', 'ot', 'or', 'is', 'me', 'in', 'en', 'ySPACE', 'nSPACE', 'BACKSPACEBACKSPACE', 'he', 'on', 'si', 'lSPACE', 'nd', 'dSPACE', 'nt', 'eSPACE', 'no', 'sSPACE', 'to', 'co', 'oSPACE', 'it', 'tSPACE', 'SPACEl', 'SPACEBACKSPACE', 'SPACEp', 'll', 'at', 'ha', 'SPACEf', 'er', 'hi', 're', 've', 'ol', 'SPACEc', 'SPACEm', 'ty', 'SPACEn', 'ar', 'ff', 'ec', 'se', 'th', 'st', 'SPACEo', 'SPACEs', 'SPACEi', 'SPACEt', 'te', 'SPACEa', 'SPACEw']\n",
        "\n",
        "feature_list_Tablet_KIT_2 = ['BACKSPACEBACKSPACE', 'me', 'si', 'or', 'ot', 'ySPACE', 'he', 'nSPACE', 'en', 'sSPACE', 'nt', 'tSPACE', 'is', 'it', 'SPACEc', 'lSPACE', 'no', 'SPACEBACKSPACE', 'in', 'nd', 'on', 'ne', 'SPACEl', 'co', 'ff', 'SPACEf', 'SPACEp', 'dSPACE', 'eSPACE', 'to', 'ar', 'SPACEm', 'st', 'ty', 've', 'SPACEn', 'th', 'ha', 'hi', 'SPACEo', 'te', 'ol', 'll', 'se', 'SPACEi', 'oSPACE', 'SPACEs', 'er', 'at', 'SPACEt', 're', 'SPACEa', 'SPACEw', 'ec']\n",
        "\n",
        "feature_list_Tablet_KIT_3 = ['or', 'ySPACE', 'er', 'si', 'ot', 'en', 'nSPACE', 'dSPACE', 'eSPACE', 'tSPACE', 'nt', 'lSPACE', 'no', 'sSPACE', 'SPACEl', 'SPACEp', 'ff', 'co', 'in', 'is', 'me', 'nd', 'ol', 'SPACEf', 'on', 'hi', 'to', 'it', 'he', 'ar', 'te', 'SPACEn', 'ty', 'SPACEBACKSPACE', 'BACKSPACEBACKSPACE', 'ne', 'SPACEm', 'st', 'SPACEc', 'th', 'SPACEt', 'at', 've', 'SPACEo', 'SPACEi', 'ha', 'll', 'oSPACE', 're', 'SPACEs', 'SPACEa', 'SPACEw', 'se', 'ec']\n",
        "\n",
        "feature_list_Tablet_KIT_4 = ['si', 'BACKSPACEBACKSPACE', 'nSPACE', 'ff', 'ySPACE', 'or', 'en', 'hi', 'nt', 'SPACEc', 'ot', 'ar', 'me', 'nd', 'no', 'is', 'lSPACE', 'on', 'co', 'tSPACE', 'SPACEl', 've', 'st', 'ha', 'ol', 'te', 'in', 'SPACEf', 'SPACEBACKSPACE', 'it', 'th', 'to', 'SPACEn', 'ty', 'll', 'SPACEm', 'he', 'er', 'dSPACE', 'SPACEp', 'oSPACE', 'SPACEo', 'ne', 'at', 'eSPACE', 'ec', 'SPACEs', 'sSPACE', 'se', 're', 'SPACEi', 'SPACEa', 'SPACEt', 'SPACEw']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH22nbBNnfC2",
        "colab_type": "text"
      },
      "source": [
        "## **Word level Features (Desktop Only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD7xWs2Ind1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_dict_advanced_word_Desktop = {'if': [2, 1, 3, 4, 6, 0, 13, 15, 10, 12, 7, 9], 'this': [6, 1, 3, 2, 8, 10, 4, 5, 12, 13, 15, 0, 7, 9, 14, 11], 'have': [6, 13, 3, 1, 0, 7, 4, 8, 15, 5, 12, 10, 9, 14, 2, 11], 'me': [4, 6, 0, 13, 15, 2, 7, 9, 10, 12, 1, 3], 'with': [6, 13, 9, 0, 12, 4, 10, 15, 7, 1, 5, 11, 8, 2, 14, 3], 'to': [4, 6, 0, 13, 15, 10, 12, 7, 9, 1, 3, 2], 'sentences': [13, 6, 5, 8, 11, 14, 3, 4, 10, 0, 1, 7, 15, 2, 12, 9], 'not': [1, 2, 3, 4, 6, 10, 12, 13, 15, 8, 11, 0, 7, 9, 5, 14], 'type': [1, 3, 11, 13, 5, 14, 4, 10, 2, 8, 12, 7, 0, 6, 15, 9], 'words': [11, 14, 8, 5, 1, 6, 9, 4, 13, 15, 3, 7, 0, 12, 10, 2], 'will': [1, 11, 5, 3, 15, 2, 13, 14, 12, 4, 9, 10, 8, 0, 6, 7], 'carefully': [1, 4, 3, 10, 0, 15, 14, 7, 13, 5, 8, 11, 9, 12, 6, 2], 'different': [6, 3, 8, 15, 9, 1, 12, 5, 14, 4, 11, 7, 13, 10, 0, 2], 'two': [1, 14, 11, 3, 8, 5, 13, 15, 7, 9, 10, 12, 4, 6, 0, 2], 'see': [14, 8, 13, 15, 3, 0, 1, 10, 12, 5, 7, 9, 4, 6, 2, 11], 'first': [1, 5, 8, 14, 6, 4, 10, 3, 0, 15, 12, 7, 2, 9, 13, 11], 'sample': [4, 5, 8, 1, 14, 13, 15, 10, 3, 0, 11, 2, 7, 6, 12, 9], 'sets': [6, 3, 1, 9, 10, 12, 7, 5, 4, 13, 8, 15, 0, 11, 14, 2], 'that': [6, 1, 3, 7, 0, 8, 10, 2, 9, 14, 5, 12, 4, 13, 15, 11], 'overlap': [1, 10, 14, 15, 6, 3, 8, 9, 11, 2, 0, 7, 5, 13, 12, 4], 'collection': [15, 6, 10, 1, 12, 14, 7, 8, 13, 4, 11, 0, 9, 3, 5, 2], 'is': [4, 6, 2, 10, 12, 1, 3, 7, 9, 0, 13, 15], 'there': [1, 3, 2, 6, 4, 8, 5, 11, 15, 10, 14, 7, 9, 13, 0, 12], 'data': [1, 3, 8, 2, 4, 5, 7, 9, 13, 0, 6, 10, 11, 12, 15, 14], 'of': [1, 3, 4, 6, 10, 12, 0, 13, 15, 7, 9, 2], 'test': [13, 11, 0, 14, 15, 5, 3, 1, 7, 4, 8, 6, 2, 10, 12, 9], 'are': [2, 5, 13, 15, 4, 6, 8, 3, 10, 12, 0, 11, 1, 14, 7, 9], 'lines': [6, 11, 5, 15, 4, 13, 10, 3, 8, 14, 12, 1, 0, 9, 2, 7], 'the': [1, 4, 6, 2, 8, 10, 12, 7, 9, 3, 13, 15, 0, 11, 5, 14], 'in': [1, 3, 4, 6, 0, 13, 15, 7, 9, 10, 12, 2], 'selected': [13, 0, 15, 5, 1, 6, 9, 3, 14, 12, 10, 8, 7, 11, 4, 2], 'a': [0, 1, 3], 'i': [0, 1, 3]}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EELnhp2Ps-IR",
        "colab_type": "text"
      },
      "source": [
        "## **Word level Features (Phone Only)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMCYjuOytCuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_dict_advanced_word_Phone = {'sample': [8, 14, 11, 5, 10, 7, 13, 0, 4, 2, 1, 6, 9, 3, 12, 15], 'data': [10, 7, 0, 13, 12, 4, 15, 2, 9, 8, 1, 3, 6, 5, 14, 11], 'is': [4, 6, 2, 7, 9, 10, 12, 0, 13, 15, 1, 3], 'the': [0, 13, 15, 2, 7, 9, 8, 11, 14, 10, 12, 5, 4, 6, 3, 1], 'this': [4, 13, 2, 1, 3, 15, 0, 11, 5, 10, 7, 12, 14, 6, 9, 8], 'of': [0, 13, 15, 7, 9, 10, 12, 4, 6, 1, 3, 2], 'are': [2, 1, 3, 0, 4, 6, 13, 15, 8, 10, 12, 5, 14, 11, 7, 9], 'to': [7, 9, 4, 6, 0, 13, 15, 10, 12, 1, 3, 2], 'if': [0, 13, 15, 2, 10, 12, 7, 9, 4, 6, 1, 3], 'a': [0, 1, 3], 'i': [0, 1, 3]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrN6G6XFtfnO",
        "colab_type": "text"
      },
      "source": [
        "## **Word level Features (Tablet Only)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSktIYcHtdDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_dict_advanced_word_Tablet = {'to': [2, 7, 9, 4, 6, 10, 12, 1, 3, 0, 13, 15], 'the': [2, 10, 12, 11, 4, 6, 1, 7, 9, 14, 0, 3, 5, 8, 13, 15]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHgffi6CBli3",
        "colab_type": "text"
      },
      "source": [
        "## **Extracting top 25 Features (Desktop Only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqawg9cBxUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''0.07513 ./KDE_plots/Desktop/Gender/std_kht/std_kht_if.png if std_kht\n",
        "0.07369 ./KDE_plots/Desktop/Gender/median_f1/median_f1_this.png this median_f1\n",
        "0.06316 ./KDE_plots/Desktop/Gender/median_f1/median_f1_have.png have median_f1\n",
        "0.06005 ./KDE_plots/Desktop/Gender/avg_f1/avg_f1_me.png me avg_f1\n",
        "0.06005 ./KDE_plots/Desktop/Gender/median_f1/median_f1_me.png me median_f1\n",
        "0.05933 ./KDE_plots/Desktop/Gender/median_f1/median_f1_with.png with median_f1\n",
        "0.05066 ./KDE_plots/Desktop/Gender/avg_f1/avg_f1_to.png to avg_f1\n",
        "0.05066 ./KDE_plots/Desktop/Gender/median_f1/median_f1_to.png to median_f1\n",
        "0.04587 ./KDE_plots/Desktop/Gender/avg_kht/avg_kht_if.png if avg_kht\n",
        "0.04587 ./KDE_plots/Desktop/Gender/median_kht/median_kht_if.png if median_kht\n",
        "0.04518 ./KDE_plots/Desktop/Gender/avg_f4/avg_f4_sentences.png sentences avg_f4\n",
        "0.04372 ./KDE_plots/Desktop/Gender/avg_kht/avg_kht_not.png not avg_kht\n",
        "0.04145 ./KDE_plots/Desktop/Gender/avg_kht/avg_kht_this.png this avg_kht\n",
        "0.04096 ./KDE_plots/Desktop/Gender/avg_f1/avg_f1_if.png if avg_f1\n",
        "0.04096 ./KDE_plots/Desktop/Gender/median_f1/median_f1_if.png if median_f1\n",
        "0.04087 ./KDE_plots/Desktop/Gender/median_f1/median_f1_sentences.png sentences median_f1\n",
        "0.04017 ./KDE_plots/Desktop/Gender/avg_kht/avg_kht_type.png type avg_kht\n",
        "0.03882 ./KDE_plots/Desktop/Gender/std_f3/std_f3_words.png words std_f3\n",
        "0.03828 ./KDE_plots/Desktop/Gender/std_f4/std_f4_words.png words std_f4\n",
        "0.03801 ./KDE_plots/Desktop/Gender/std_f2/std_f2_words.png words std_f2\n",
        "0.03673 ./KDE_plots/Desktop/Gender/avg_kht/avg_kht_will.png will avg_kht\n",
        "0.03597 ./KDE_plots/Desktop/Gender/median_kht/median_kht_this.png this median_kht\n",
        "0.03567 ./KDE_plots/Desktop/Gender/avg_kht/avg_kht_carefully.png carefully avg_kht\n",
        "0.03547 ./KDE_plots/Desktop/Gender/wht/wht_if.png if wht\n",
        "0.03547 ./KDE_plots/Desktop/Gender/avg_f4/avg_f4_if.png if avg_f4'''\n",
        "word_feature_id_mapping = {1: 'wht', 2: 'avg_kht', 3: 'std_kht', 4: 'median_kht', 5: 'avg_f1', 6: 'std_f1', 7: 'median_f1', 8: 'avg_f2', 9: 'std_f2', 10: 'median_f2', 11: 'avg_f3', 12: 'std_f3', 13: 'median_f3', 14: 'avg_f4', 15: 'std_f4', 16: 'median_f4'}\n",
        "\n",
        "top_feature_advanced_word_Desktop_only_map = {}\n",
        "top_feature_advanced_word_Desktop_only_map[\"if\"] = [2, 1, 3, 4, 6, 0, 13]\n",
        "top_feature_advanced_word_Desktop_only_map[\"this\"] = [6, 1, 3]\n",
        "top_feature_advanced_word_Desktop_only_map[\"have\"] = [6]\n",
        "top_feature_advanced_word_Desktop_only_map[\"me\"] = [4, 6]\n",
        "top_feature_advanced_word_Desktop_only_map[\"with\"] = [6]\n",
        "top_feature_advanced_word_Desktop_only_map[\"to\"] = [4, 6]\n",
        "top_feature_advanced_word_Desktop_only_map[\"sentences\"] = [13, 6]\n",
        "top_feature_advanced_word_Desktop_only_map[\"not\"] = [1]\n",
        "top_feature_advanced_word_Desktop_only_map[\"type\"] = [1]\n",
        "top_feature_advanced_word_Desktop_only_map[\"words\"] = [11, 8, 14]\n",
        "top_feature_advanced_word_Desktop_only_map[\"will\"] = [1]\n",
        "top_feature_advanced_word_Desktop_only_map[\"carefully\"] = [1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_3Vj05DB0rt",
        "colab_type": "text"
      },
      "source": [
        "## **Extracting top 25 Features (Phone Only)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFUX-xgZGpS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''0.13533 ./KDE_plots/Phone/Gender/std_f2/std_f2_sample.png sample std_f2\n",
        "0.13211 ./KDE_plots/Phone/Gender/std_f4/std_f4_sample.png sample std_f4\n",
        "0.12659 ./KDE_plots/Phone/Gender/std_f3/std_f3_sample.png sample std_f3\n",
        "0.12489 ./KDE_plots/Phone/Gender/std_f1/std_f1_sample.png sample std_f1\n",
        "0.07055 ./KDE_plots/Phone/Gender/avg_f3/avg_f3_sample.png sample avg_f3\n",
        "0.06776 ./KDE_plots/Phone/Gender/avg_f2/avg_f2_sample.png sample avg_f2\n",
        "0.06308 ./KDE_plots/Phone/Gender/avg_f4/avg_f4_sample.png sample avg_f4\n",
        "0.05921 ./KDE_plots/Phone/Gender/wht/wht_sample.png sample wht\n",
        "0.05472 ./KDE_plots/Phone/Gender/avg_f1/avg_f1_sample.png sample avg_f1\n",
        "0.03678 ./KDE_plots/Phone/Gender/avg_f3/avg_f3_data.png data avg_f3\n",
        "0.03271 ./KDE_plots/Phone/Gender/avg_f2/avg_f2_data.png data avg_f2\n",
        "0.03012 ./KDE_plots/Phone/Gender/avg_f1/avg_f1_is.png is avg_f1\n",
        "0.03012 ./KDE_plots/Phone/Gender/median_f1/median_f1_is.png is median_f1\n",
        "0.02885 ./KDE_plots/Phone/Gender/wht/wht_data.png data wht\n",
        "0.02593 ./KDE_plots/Phone/Gender/avg_f4/avg_f4_data.png data avg_f4\n",
        "0.02493 ./KDE_plots/Phone/Gender/median_f3/median_f3_data.png data median_f3\n",
        "0.02488 ./KDE_plots/Phone/Gender/std_kht/std_kht_is.png is std_kht\n",
        "0.02463 ./KDE_plots/Phone/Gender/avg_f1/avg_f1_data.png data avg_f1\n",
        "0.02393 ./KDE_plots/Phone/Gender/wht/wht_the.png the wht\n",
        "0.02333 ./KDE_plots/Phone/Gender/avg_f4/avg_f4_the.png the avg_f4\n",
        "0.02333 ./KDE_plots/Phone/Gender/median_f4/median_f4_the.png the median_f4\n",
        "0.02216 ./KDE_plots/Phone/Gender/avg_f2/avg_f2_is.png is avg_f2\n",
        "0.02216 ./KDE_plots/Phone/Gender/median_f2/median_f2_is.png is median_f2\n",
        "0.02151 ./KDE_plots/Phone/Gender/avg_f3/avg_f3_is.png is avg_f3\n",
        "0.02151 ./KDE_plots/Phone/Gender/median_f3/median_f3_is.png is median_f3'''\n",
        "\n",
        "\n",
        "word_feature_id_mapping = {1: 'wht', 2: 'avg_kht', 3: 'std_kht', 4: 'median_kht', 5: 'avg_f1', 6: 'std_f1', 7: 'median_f1', 8: 'avg_f2', 9: 'std_f2', 10: 'median_f2', 11: 'avg_f3', 12: 'std_f3', 13: 'median_f3', 14: 'avg_f4', 15: 'std_f4', 16: 'median_f4'}\n",
        "\n",
        "top_feature_advanced_word_Phone_only_map = {}\n",
        "top_feature_advanced_word_Phone_only_map[\"sample\"] = [0, 4, 5, 8, 11, 14, 7, 10, 13]\n",
        "top_feature_advanced_word_Phone_only_map[\"data\"] = [0, 4, 7, 10, 13, 12]\n",
        "top_feature_advanced_word_Phone_only_map[\"is\"] = [2, 4, 6, 7, 9, 10, 12]\n",
        "top_feature_advanced_word_Phone_only_map[\"the\"] = [0, 13, 15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaBI5XD-RHXg",
        "colab_type": "text"
      },
      "source": [
        "## **Extracting top 25 Features (Tablet Only)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFDWqeKnRJqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''0.02673 ./KDE_plots/Tablet/Gender/std_kht/std_kht_to.png to std_kht\n",
        "0.0128 ./KDE_plots/Tablet/Gender/std_kht/std_kht_the.png the std_kht\n",
        "0.01215 ./KDE_plots/Tablet/Gender/avg_f3/avg_f3_the.png the avg_f3\n",
        "0.01215 ./KDE_plots/Tablet/Gender/median_f3/median_f3_the.png the median_f3\n",
        "0.01017 ./KDE_plots/Tablet/Gender/avg_f2/avg_f2_to.png to avg_f2\n",
        "0.01017 ./KDE_plots/Tablet/Gender/median_f2/median_f2_to.png to median_f2\n",
        "0.00932 ./KDE_plots/Tablet/Gender/avg_f1/avg_f1_to.png to avg_f1\n",
        "0.00932 ./KDE_plots/Tablet/Gender/median_f1/median_f1_to.png to median_f1\n",
        "0.00847 ./KDE_plots/Tablet/Gender/std_f3/std_f3_the.png the std_f3\n",
        "0.00846 ./KDE_plots/Tablet/Gender/avg_f1/avg_f1_the.png the avg_f1\n",
        "0.00846 ./KDE_plots/Tablet/Gender/median_f1/median_f1_the.png the median_f1\n",
        "0.00629 ./KDE_plots/Tablet/Gender/avg_f3/avg_f3_to.png to avg_f3\n",
        "0.00629 ./KDE_plots/Tablet/Gender/median_f3/median_f3_to.png to median_f3\n",
        "0.00601 ./KDE_plots/Tablet/Gender/avg_kht/avg_kht_the.png the avg_kht\n",
        "0.00545 ./KDE_plots/Tablet/Gender/avg_f2/avg_f2_the.png the avg_f2\n",
        "0.00545 ./KDE_plots/Tablet/Gender/median_f2/median_f2_the.png the median_f2\n",
        "0.00509 ./KDE_plots/Tablet/Gender/std_f4/std_f4_the.png the std_f4\n",
        "0.00492 ./KDE_plots/Tablet/Gender/wht/wht_the.png the wht\n",
        "0.00447 ./KDE_plots/Tablet/Gender/median_kht/median_kht_the.png the median_kht\n",
        "0.0044 ./KDE_plots/Tablet/Gender/std_f1/std_f1_the.png the std_f1\n",
        "0.00418 ./KDE_plots/Tablet/Gender/std_f2/std_f2_the.png the std_f2\n",
        "0.00383 ./KDE_plots/Tablet/Gender/avg_kht/avg_kht_to.png to avg_kht\n",
        "0.00383 ./KDE_plots/Tablet/Gender/median_kht/median_kht_to.png to median_kht\n",
        "0.00328 ./KDE_plots/Tablet/Gender/wht/wht_to.png to wht\n",
        "0.00328 ./KDE_plots/Tablet/Gender/avg_f4/avg_f4_to.png to avg_f4'''\n",
        "\n",
        "word_feature_id_mapping = {1: 'wht', 2: 'avg_kht', 3: 'std_kht', 4: 'median_kht', 5: 'avg_f1', 6: 'std_f1', 7: 'median_f1', 8: 'avg_f2', 9: 'std_f2', 10: 'median_f2', 11: 'avg_f3', 12: 'std_f3', 13: 'median_f3', 14: 'avg_f4', 15: 'std_f4', 16: 'median_f4'}\n",
        "\n",
        "top_feature_advanced_word_Tablet_only_map = {}\n",
        "top_feature_advanced_word_Tablet_only_map[\"to\"] = [0, 1, 2, 3, 4, 6, 7, 9, 10, 12, 13]\n",
        "top_feature_advanced_word_Tablet_only_map[\"the\"] = [0, 3, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYauSafBYN2q",
        "colab_type": "text"
      },
      "source": [
        "## **Extracting top 25 Features (Combined = Desktop + Phone)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB1rmRVnXtob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''0.13533 ./KDE_plots/Phone/Gender/std_f2/std_f2_sample.png sample std_f2\n",
        "0.13211 ./KDE_plots/Phone/Gender/std_f4/std_f4_sample.png sample std_f4\n",
        "0.12659 ./KDE_plots/Phone/Gender/std_f3/std_f3_sample.png sample std_f3\n",
        "0.12489 ./KDE_plots/Phone/Gender/std_f1/std_f1_sample.png sample std_f1\n",
        "0.07513 ./KDE_plots/Desktop/Gender/std_kht/std_kht_if.png if std_kht\n",
        "0.07369 ./KDE_plots/Desktop/Gender/median_f1/median_f1_this.png this median_f1\n",
        "0.07055 ./KDE_plots/Phone/Gender/avg_f3/avg_f3_sample.png sample avg_f3\n",
        "0.06776 ./KDE_plots/Phone/Gender/avg_f2/avg_f2_sample.png sample avg_f2\n",
        "0.06316 ./KDE_plots/Desktop/Gender/median_f1/median_f1_have.png have median_f1\n",
        "0.06308 ./KDE_plots/Phone/Gender/avg_f4/avg_f4_sample.png sample avg_f4\n",
        "0.06005 ./KDE_plots/Desktop/Gender/avg_f1/avg_f1_me.png me avg_f1\n",
        "0.06005 ./KDE_plots/Desktop/Gender/median_f1/median_f1_me.png me median_f1\n",
        "0.05933 ./KDE_plots/Desktop/Gender/median_f1/median_f1_with.png with median_f1\n",
        "0.05921 ./KDE_plots/Phone/Gender/wht/wht_sample.png sample wht\n",
        "0.05472 ./KDE_plots/Phone/Gender/avg_f1/avg_f1_sample.png sample avg_f1\n",
        "0.05066 ./KDE_plots/Desktop/Gender/avg_f1/avg_f1_to.png to avg_f1\n",
        "0.05066 ./KDE_plots/Desktop/Gender/median_f1/median_f1_to.png to median_f1\n",
        "0.04587 ./KDE_plots/Desktop/Gender/avg_kht/avg_kht_if.png if avg_kht\n",
        "0.04587 ./KDE_plots/Desktop/Gender/median_kht/median_kht_if.png if median_kht\n",
        "0.04518 ./KDE_plots/Desktop/Gender/avg_f4/avg_f4_sentences.png sentences avg_f4\n",
        "0.04372 ./KDE_plots/Desktop/Gender/avg_kht/avg_kht_not.png not avg_kht\n",
        "0.04145 ./KDE_plots/Desktop/Gender/avg_kht/avg_kht_this.png this avg_kht\n",
        "0.04096 ./KDE_plots/Desktop/Gender/avg_f1/avg_f1_if.png if avg_f1\n",
        "0.04096 ./KDE_plots/Desktop/Gender/median_f1/median_f1_if.png if median_f1\n",
        "0.04087 ./KDE_plots/Desktop/Gender/median_f1/median_f1_sentences.png sentences median_f1'''\n",
        "word_feature_id_mapping = {1: 'wht', 2: 'avg_kht', 3: 'std_kht', 4: 'median_kht', 5: 'avg_f1',\n",
        "                           6: 'std_f1', 7: 'median_f1', 8: 'avg_f2', 9: 'std_f2', 10: 'median_f2',\n",
        "                           11: 'avg_f3', 12: 'std_f3', 13: 'median_f3', 14: 'avg_f4', 15: 'std_f4', \n",
        "                           16: 'median_f4'}\n",
        "\n",
        "\n",
        "top_feature_advanced_word_Desktop_map_DP = {}\n",
        "top_feature_advanced_word_Desktop_map_DP[\"if\"] = [1, 2, 3, 4, 6]\n",
        "top_feature_advanced_word_Desktop_map_DP[\"this\"] = [1, 6]\n",
        "top_feature_advanced_word_Desktop_map_DP[\"have\"] = [6]\n",
        "top_feature_advanced_word_Desktop_map_DP[\"me\"] = [4, 6]\n",
        "top_feature_advanced_word_Desktop_map_DP[\"with\"] = [6]\n",
        "top_feature_advanced_word_Desktop_map_DP[\"to\"] = [4, 6]\n",
        "top_feature_advanced_word_Desktop_map_DP[\"sentences\"] = [6, 13]\n",
        "top_feature_advanced_word_Desktop_map_DP[\"not\"] = [1]\n",
        "\n",
        "top_feature_advanced_word_Phone_map_DP = {}\n",
        "top_feature_advanced_word_Phone_map_DP[\"sample\"] = [0, 4, 5, 7, 8, 10, 11, 13, 14]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skkc7GVCI05-",
        "colab_type": "text"
      },
      "source": [
        "## **Code to extract top 25 word level Features (Combined)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie959lVOIxec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of top 25 features and their equivalent position\n",
        "top_feature_advanced_word_Desktop_map = {}\n",
        "top_feature_advanced_word_Desktop_map[\"selected\"] = [0,7,10]\n",
        "top_feature_advanced_word_Desktop_map[\"me\"] = [4,6]\n",
        "top_feature_advanced_word_Desktop_map[\"if\"] = [4,6]\n",
        "top_feature_advanced_word_Desktop_map[\"that\"] = [4]\n",
        "top_feature_advanced_word_Desktop_map[\"sample\"] = [4]\n",
        "top_feature_advanced_word_Desktop_map[\"test\"] = [4]\n",
        "top_feature_advanced_word_Desktop_map[\"have\"] = [4]\n",
        "top_feature_advanced_word_Desktop_map[\"data\"] = [1,4]\n",
        "top_feature_advanced_word_Desktop_map[\"with\"] = [13]\n",
        "top_feature_advanced_word_Desktop_map[\"will\"] = [13]\n",
        "\n",
        "top_feature_advanced_word_Phone_map = {}\n",
        "top_feature_advanced_word_Phone_map[\"sample\"] = [7,14]\n",
        "\n",
        "top_feature_advanced_word_Tablet_map = {}\n",
        "top_feature_advanced_word_Tablet_map[\"have\"] = [4,6,9,12]\n",
        "top_feature_advanced_word_Tablet_map[\"is\"] = [4,6,10,12]\n",
        "\n",
        "# Calculated median of required features\n",
        "def top_feature_advanced_word(pickle_file, feature_dict):\n",
        "    temp_features = pickle.load(open(pickle_file, 'rb'))\n",
        "    selected_top_feature = []\n",
        "    top_feature_list = [[0 for x in range(len(temp_features))] for x in range(116)] \n",
        "    #for key in feature_dict:\n",
        "    for key1 in temp_features:\n",
        "        if key1 == 117:\n",
        "            break\n",
        "        temp = [] \n",
        "        for key2 in temp_features[key1]:\n",
        "            for key in feature_dict:\n",
        "                if key2 == key:              \n",
        "                    for i in (feature_dict[key]):\n",
        "                        # Removing outliers\n",
        "                        temp_without_outlier = remove_outliers(np.array(temp_features[key1][key2])[:,i])\n",
        "\n",
        "                        # Median feature\n",
        "                        #temp.append(np.median(temp_without_outlier))\n",
        "\n",
        "                        '''# Mean feature\n",
        "                        temp.append(np.mean(temp_without_outlier))\n",
        "\n",
        "                        #IQR feature\n",
        "                        a = np.asarray(temp_without_outlier)\n",
        "                        upper_quartile = np.percentile(a, 75)\n",
        "                        lower_quartile = np.percentile(a, 25)\n",
        "                        IQR = (upper_quartile - lower_quartile) * 1.5\n",
        "                        temp.append(IQR)\n",
        "                        temp.append(upper_quartile)\n",
        "                        temp.append(lower_quartile)\n",
        "\n",
        "                        # Kurtosis feature\n",
        "                        temp.append(kurtosis(temp_without_outlier))\n",
        "\n",
        "                        # Skew features\n",
        "                        temp.append(skew(temp_without_outlier))'''\n",
        "\n",
        "                        # Mean-Median\n",
        "                        temp.append(np.mean(temp_without_outlier) - np.median(temp_without_outlier))\n",
        "                    break\n",
        "            top_feature_list[key1-1] = list(temp)          \n",
        "    return top_feature_list\n",
        "\n",
        "# Combines data from multiple devices (desktop, phone, tablet)\n",
        "def combine_top_advanced_word():\n",
        "    desktop_features = top_feature_advanced_word('desktop_advanced_word_feature_dictionary.pickle', top_feature_advanced_word_Desktop_map)\n",
        "    phone_features = top_feature_advanced_word('phone_advanced_word_feature_dictionary.pickle', top_feature_advanced_word_Phone_map)\n",
        "    tablet_features = top_feature_advanced_word('tablet_advanced_word_feature_dictionary.pickle', top_feature_advanced_word_Tablet_map)\n",
        "    return np.concatenate((np.array(desktop_features), np.array(phone_features), np.array(tablet_features)), axis=1)\n",
        "\n",
        "def combine_top_advanced_word_desktop_only():\n",
        "    desktop_features = top_feature_advanced_word('desktop_advanced_word_feature_dictionary.pickle', top_feature_advanced_word_Desktop_only_map)\n",
        "    return np.array(desktop_features)\n",
        "\n",
        "def combine_top_advanced_word_phone_only():\n",
        "    desktop_features = top_feature_advanced_word('desktop_advanced_word_feature_dictionary.pickle', top_feature_advanced_word_Phone_only_map)\n",
        "    return np.array(desktop_features)\n",
        "\n",
        "def combine_top_advanced_word_tablet_only():\n",
        "    desktop_features = top_feature_advanced_word('desktop_advanced_word_feature_dictionary.pickle', top_feature_advanced_word_Tablet_only_map)\n",
        "    return np.array(desktop_features)\n",
        "\n",
        "def combine_top_advanced_word_desktop_and_phone_only():\n",
        "    desktop_features = top_feature_advanced_word('desktop_advanced_word_feature_dictionary.pickle', top_feature_advanced_word_Desktop_map_DP)\n",
        "    phone_features = top_feature_advanced_word('phone_advanced_word_feature_dictionary.pickle', top_feature_advanced_word_Phone_map_DP)\n",
        "    return np.concatenate((np.array(desktop_features), np.array(phone_features)), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLKx5poJmm1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utilities to get the most commonly occurring words and corresponding features for all users\n",
        "word_feature_id_mapping = {1: 'wht', 2: 'avg_kht', 3: 'std_kht', 4: 'median_kht', 5: 'avg_f1', 6: 'std_f1', 7: 'median_f1', 8: 'avg_f2', 9: 'std_f2', 10: 'median_f2', 11: 'avg_f3', 12: 'std_f3', 13: 'median_f3', 14: 'avg_f4', 15: 'std_f4', 16: 'median_f4'}\n",
        "\n",
        "def get_key_array_for_user(user_key, user_dict):\n",
        "    key_array_lengths = []\n",
        "    for key in user_dict[user_key]:\n",
        "        key_array_lengths.append((len(user_dict[user_key][key]), key))\n",
        "    final_keys_array = []\n",
        "    for key in key_array_lengths:\n",
        "        final_keys_array.append(key[1])\n",
        "    return final_keys_array\n",
        "\n",
        "def get_top_word_keys(device):\n",
        "    advanced_word_feat_dict = pickle.load(open(device+'_advanced_word_feature_dictionary.pickle', 'rb'))\n",
        "    final_key_set = get_key_array_for_user(1, advanced_word_feat_dict)\n",
        "    for user_id in advanced_word_feat_dict:\n",
        "        user_key_array = get_key_array_for_user(user_id, advanced_word_feat_dict)\n",
        "        final_key_set = set(user_key_array).intersection(final_key_set)\n",
        "    return final_key_set\n",
        "\n",
        "def get_advanced_word_values_given_user_and_key(advanced_word_feat_dict, user_key, key, word_feat_id):\n",
        "    try:\n",
        "        temp = list(np.asarray(advanced_word_feat_dict[user_key+1][key])[:, word_feat_id])\n",
        "        for k in temp:\n",
        "            if(math.isnan(k)):\n",
        "                return int(0)\n",
        "        return abs(np.median(np.asarray(advanced_word_feat_dict[user_key+1][key])[:, word_feat_id]))\n",
        "    except KeyError as e:\n",
        "        return None\n",
        "\n",
        "def get_advanced_word_features(device):\n",
        "    \"\"\" Input: All feature dictionary Output: Feature matrix with unique columns\"\"\"\n",
        "    features = pickle.load(open(device+'_advanced_word_feature_dictionary.pickle', 'rb'))\n",
        "\n",
        "    # Getting unique columns by removing repeated keys\n",
        "    feature_set = list(get_top_word_keys(device))\n",
        "    val = []\n",
        "    rows, cols = (len(features), len(feature_set))\n",
        "    feature_vector = [[0 for x in range(cols * 16)] for x in range(rows)] \n",
        "\n",
        "    for i, key in enumerate(feature_set):\n",
        "        for j in range(16):\n",
        "            val.append(get_advanced_word_values_given_user_and_key(features, i, key, j))\n",
        "        feature_vector[i] = val\n",
        "\n",
        "    return feature_vector\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIfK1VKYvuel",
        "colab_type": "text"
      },
      "source": [
        "## **Feature Matrix for Desktop (KHT (Unigraph) + KIT (Digraph) + Word)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry-niGWewGgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return all the desktop features for free text\n",
        "def get_desktop_features():\n",
        "    desktop_features_KHT = top_feature_KIT(\"desktop_kht_feature_dictionary.pickle\",feature_list_Desktop_KHT)\n",
        "    desktop_features_KIT_1 = top_feature_KIT(\"desktop_kit_feature_f1_dictionary.pickle\", feature_list_Desktop_KIT_1)\n",
        "    desktop_features_KIT_2 = top_feature_KIT(\"desktop_kit_feature_f2_dictionary.pickle\", feature_list_Desktop_KIT_2)\n",
        "    desktop_features_KIT_3 = top_feature_KIT(\"desktop_kit_feature_f3_dictionary.pickle\", feature_list_Desktop_KIT_3)\n",
        "    desktop_features_KIT_4 = top_feature_KIT(\"desktop_kit_feature_f4_dictionary.pickle\", feature_list_Desktop_KIT_4)\n",
        "\n",
        "    desktop_features_advanced = top_feature_advanced_word('desktop_advanced_word_feature_dictionary.pickle', feature_dict_advanced_word_Desktop)\n",
        "    return np.concatenate((np.array(desktop_features_KHT), np.array(desktop_features_advanced), np.array(desktop_features_KIT_1), np.array(desktop_features_KIT_2), np.array(desktop_features_KIT_3), np.array(desktop_features_KIT_4)), axis=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO1Us2SK67D-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return all the desktop features for fixed text\n",
        "def get_desktop_features_fixed():\n",
        "    desktop_features_KHT_fixed = top_feature_KIT(\"desktop_kht_feature_dictionary_fixed.pickle\",feature_list_Desktop_KHT)\n",
        "    desktop_features_KIT_1_fixed = top_feature_KIT(\"desktop_kit_feature_f1_dictionary_fixed.pickle\", feature_list_Desktop_KIT_1)\n",
        "    desktop_features_KIT_2_fixed = top_feature_KIT(\"desktop_kit_feature_f2_dictionary_fixed.pickle\", feature_list_Desktop_KIT_2)\n",
        "    desktop_features_KIT_3_fixed = top_feature_KIT(\"desktop_kit_feature_f3_dictionary_fixed.pickle\", feature_list_Desktop_KIT_3)\n",
        "    desktop_features_KIT_4_fixed = top_feature_KIT(\"desktop_kit_feature_f4_dictionary_fixed.pickle\", feature_list_Desktop_KIT_4)\n",
        "\n",
        "    desktop_features_advanced_fixed = top_feature_advanced_word('desktop_advanced_word_feature_dictionary_fixed.pickle', feature_dict_advanced_word_Desktop)\n",
        "    return np.concatenate((np.array(desktop_features_KHT_fixed), np.array(desktop_features_advanced_fixed), np.array(desktop_features_KIT_1_fixed), np.array(desktop_features_KIT_2_fixed), np.array(desktop_features_KIT_3_fixed), np.array(desktop_features_KIT_4_fixed)), axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIQM8mtT6rI-",
        "colab_type": "text"
      },
      "source": [
        "## **Feature Matrix for Phone (KHT (Unigraph) + KIT (Digraph) + Word)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS6ECMns6twy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return all the phone features for free text\n",
        "def get_phone_features():\n",
        "    phone_features_KHT = top_feature_KIT(\"phone_kht_feature_dictionary.pickle\",feature_list_Phone_KHT)\n",
        "    phone_features_KIT_1 = top_feature_KIT(\"phone_kit_feature_f1_dictionary.pickle\", feature_list_Phone_KIT_1)\n",
        "    phone_features_KIT_2 = top_feature_KIT(\"phone_kit_feature_f2_dictionary.pickle\", feature_list_Phone_KIT_2)\n",
        "    phone_features_KIT_3 = top_feature_KIT(\"phone_kit_feature_f3_dictionary.pickle\", feature_list_Phone_KIT_3)\n",
        "    phone_features_KIT_4 = top_feature_KIT(\"phone_kit_feature_f4_dictionary.pickle\", feature_list_Phone_KIT_4)\n",
        "\n",
        "    phone_features_advanced = top_feature_advanced_word('phone_advanced_word_feature_dictionary.pickle', feature_dict_advanced_word_Phone)\n",
        "    return np.concatenate((np.array(phone_features_KHT), np.array(phone_features_advanced), np.array(phone_features_KIT_1), np.array(phone_features_KIT_2), np.array(phone_features_KIT_3), np.array(phone_features_KIT_4)), axis=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEyJl0NM7tkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return all the phone features for fixed text\n",
        "def get_phone_features_fixed():\n",
        "    phone_features_KHT_fixed = top_feature_KIT(\"phone_kht_feature_dictionary_fixed.pickle\",feature_list_Phone_KHT)\n",
        "    phone_features_KIT_1_fixed = top_feature_KIT(\"phone_kit_feature_f1_dictionary_fixed.pickle\", feature_list_Phone_KIT_1)\n",
        "    phone_features_KIT_2_fixed = top_feature_KIT(\"phone_kit_feature_f2_dictionary_fixed.pickle\", feature_list_Phone_KIT_2)\n",
        "    phone_features_KIT_3_fixed = top_feature_KIT(\"phone_kit_feature_f3_dictionary_fixed.pickle\", feature_list_Phone_KIT_3)\n",
        "    phone_features_KIT_4_fixed = top_feature_KIT(\"phone_kit_feature_f4_dictionary_fixed.pickle\", feature_list_Phone_KIT_4)\n",
        "\n",
        "    phone_features_advanced_fixed = top_feature_advanced_word('phone_advanced_word_feature_dictionary.pickle', feature_dict_advanced_word_Phone)\n",
        "    return np.concatenate((np.array(phone_features_KHT_fixed), np.array(phone_features_advanced_fixed), np.array(phone_features_KIT_1_fixed), np.array(phone_features_KIT_2_fixed), np.array(phone_features_KIT_3_fixed), np.array(phone_features_KIT_4_fixed)), axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8mEjX9g_EJL",
        "colab_type": "text"
      },
      "source": [
        "## **Feature Matrix for Tablet (KHT (Unigraph) + KIT (Digraph) + Word)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0C7iymW_HAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return all the tablet features for free text\n",
        "def get_tablet_features():\n",
        "    tablet_features_KHT = top_feature_KIT(\"phone_kht_feature_dictionary.pickle\",feature_list_Tablet_KHT)\n",
        "    tablet_features_KIT_1 = top_feature_KIT(\"phone_kit_feature_f1_dictionary.pickle\", feature_list_Tablet_KIT_1)\n",
        "    tablet_features_KIT_2 = top_feature_KIT(\"phone_kit_feature_f2_dictionary.pickle\", feature_list_Tablet_KIT_2)\n",
        "    tablet_features_KIT_3 = top_feature_KIT(\"phone_kit_feature_f3_dictionary.pickle\", feature_list_Tablet_KIT_3)\n",
        "    tablet_features_KIT_4 = top_feature_KIT(\"phone_kit_feature_f4_dictionary.pickle\", feature_list_Tablet_KIT_4)\n",
        "\n",
        "    tablet_features_advanced = top_feature_advanced_word('tablet_advanced_word_feature_dictionary.pickle', feature_dict_advanced_word_Tablet)\n",
        "    return np.concatenate((np.array(tablet_features_KHT), np.array(tablet_features_advanced), np.array(tablet_features_KIT_1), np.array(tablet_features_KIT_2), np.array(tablet_features_KIT_3), np.array(tablet_features_KIT_4)), axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyKJsiYp8LWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return all the tablet features for fixed text\n",
        "def get_tablet_features_fixed():\n",
        "    tablet_features_KHT_fixed = top_feature_KIT(\"phone_kht_feature_dictionary_fixed.pickle\",feature_list_Tablet_KHT)\n",
        "    tablet_features_KIT_1_fixed = top_feature_KIT(\"phone_kit_feature_f1_dictionary_fixed.pickle\", feature_list_Tablet_KIT_1)\n",
        "    tablet_features_KIT_2_fixed = top_feature_KIT(\"phone_kit_feature_f2_dictionary_fixed.pickle\", feature_list_Tablet_KIT_2)\n",
        "    tablet_features_KIT_3_fixed = top_feature_KIT(\"phone_kit_feature_f3_dictionary_fixed.pickle\", feature_list_Tablet_KIT_3)\n",
        "    tablet_features_KIT_4_fixed = top_feature_KIT(\"phone_kit_feature_f4_dictionary_fixed.pickle\", feature_list_Tablet_KIT_4)\n",
        "\n",
        "    tablet_features_advanced_fixed = top_feature_advanced_word('tablet_advanced_word_feature_dictionary.pickle', feature_dict_advanced_word_Tablet)\n",
        "    return np.concatenate((np.array(tablet_features_KHT_fixed), np.array(tablet_features_advanced_fixed), np.array(tablet_features_KIT_1_fixed), np.array(tablet_features_KIT_2_fixed), np.array(tablet_features_KIT_3_fixed), np.array(tablet_features_KIT_4_fixed)), axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkRic5rTJC2M",
        "colab_type": "text"
      },
      "source": [
        "## **Feature Matrix for Combined (KHT (Unigraph) + KIT (Digraph) + Word)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujVDaYeuJPTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return all the combined (desktop+phone+tablet) features for free text\n",
        "def get_combined_features():\n",
        "    desktop_features_combined = get_desktop_features()\n",
        "    phone_features_combined = get_phone_features()\n",
        "    tablet_features_combined = get_tablet_features()\n",
        "    return np.concatenate((np.array(desktop_features_combined), np.array(phone_features_combined), np.array(tablet_features_combined)), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF8qoGZJ8jJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return all the combined (desktop+phone+tablet) features for fixed text\n",
        "def get_combined_features_fixed():\n",
        "    desktop_features_combined_fixed = get_desktop_features_fixed()\n",
        "    phone_features_combined_fixed = get_phone_features_fixed()\n",
        "    tablet_features_combined_fixed = get_tablet_features_fixed()\n",
        "    return np.concatenate((np.array(desktop_features_combined_fixed), np.array(phone_features_combined_fixed), np.array(tablet_features_combined_fixed)), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Wlm3UpfcXW",
        "colab_type": "text"
      },
      "source": [
        "# **Machine Learning Models: Classification tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyUZJPIg4Si3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install skfeature-chappers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcSbYwNTTdmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from skfeature.function.information_theoretical_based import MRMR\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "def compare_classification(label_name, feature_type, top_n_features, model):\n",
        "''' Function to process the data, apply oversampling techniques (SMOTE) and run the classification model specified using GridSearchCV\n",
        "Input:  label_name: The task to be performed (Gender, Major/Minor, Typing Style)\n",
        "        feature_type: The feature set to be used (Desktop, Phone, Tablet, Combined)\n",
        "        top_n_features: Thu number of features to be selected using Mutual Info criterion\n",
        "        model: The ML model to train and evaluate\n",
        "Output: accuracy scores, best hyperparameters of the gridsearch run\n",
        "'''\n",
        "    # Creating class label vector using metadata\n",
        "    demographics_data_frame = pd.read_csv(\"Demographics.csv\")\n",
        "    Y_values = demographics_data_frame[label_name].to_numpy()\n",
        "    Y_vector = np.asarray(Y_values)\n",
        "\n",
        "    if(label_name == \"Typing Style\"):\n",
        "        for i in range(117):\n",
        "            if(Y_vector[i] == 'a'):\n",
        "                Y_vector[i] = 0\n",
        "            if(Y_vector[i] == 'b'):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 2\n",
        "\n",
        "    if(label_name == \"Major/Minor\"):\n",
        "        string1 = \"Computer\"\n",
        "        string2 = \"CS\"\n",
        "        for i, v in enumerate(Y_vector):\n",
        "            if (type(Y_vector[i]) == float):\n",
        "                Y_vector[i] = 1\n",
        "                continue\n",
        "            if \"LEFT\" in Y_vector[i]:\n",
        "                Y_vector[i] = 0\n",
        "            if string1 in v or string2 in v:\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    if (label_name == \"Gender\" or label_name == \"Ethnicity\"):\n",
        "        for i in range(116):\n",
        "            if(Y_values[i] == 'M' or Y_values[i] == \"Asian\"):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    Y_vector = Y_vector[:-1]\n",
        "    Y_values = Y_values[:-1]\n",
        "    Y_vector = Y_vector.astype('int')\n",
        "\n",
        "    X_matrix = feature_type\n",
        "\n",
        "    # Normalizing features and selecting top 20\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_matrix = min_max_scaler.fit_transform(X_matrix)\n",
        "    X_matrix = preprocessing.scale(X_matrix)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    X_matrix_new = SelectKBest(mutual_info_classif, k=top_n_features).fit_transform(X_matrix, Y_vector)\n",
        "    X_matrix_new, Y_vector = SMOTE(kind='svm').fit_sample(X_matrix_new, Y_vector)\n",
        "\n",
        "    # Split the dataset in two equal parts\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\n",
        "    if model == \"SVM\":    \n",
        "        # Set the parameters by cross-validation\n",
        "        tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
        "                     'C': [0.1, 1, 10, 100, 1000, 10000]},\n",
        "                     {'kernel': ['poly'], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
        "                     'C': [0.1, 1, 10, 100, 1000, 10000]},\n",
        "                     {'kernel': ['sigmoid'], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
        "                     'C': [0.1, 1, 10, 100, 1000, 10000]},\n",
        "                    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000,10000]}]\n",
        "\n",
        "        clf = GridSearchCV(\n",
        "            SVC(), tuned_parameters, scoring='accuracy', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return accuracy_score(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "\n",
        "    if model == \"DTree\":\n",
        "        tuned_parameters = {\n",
        "            'criterion':[\"gini\", \"entropy\"],\n",
        "            'splitter': [\"best\", \"random\"],\n",
        "            'max_leaf_nodes': list(range(2, 100)),\n",
        "            'min_samples_split': [2, 3, 4, 6, 8, 10],\n",
        "        }\n",
        "        clf = GridSearchCV(\n",
        "            DecisionTreeClassifier(random_state=42), tuned_parameters, scoring='accuracy', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return accuracy_score(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "\n",
        "    if model == \"RForest\":\n",
        "        tuned_parameters = {'bootstrap': [True, False],\n",
        "            'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        "            'max_features': ['auto', 'sqrt'],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'n_estimators': [200, 400, 600, 800, 1000]\n",
        "        }\n",
        "        clf = GridSearchCV(\n",
        "            RandomForestClassifier(), tuned_parameters, scoring='accuracy', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return accuracy_score(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "\n",
        "    if model == \"XGBoost\":\n",
        "        tuned_parameters = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }\n",
        "        clf = GridSearchCV(\n",
        "            xgb.XGBClassifier(), tuned_parameters, scoring='accuracy', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return accuracy_score(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "\n",
        "    if model == \"ABoost\":\n",
        "        tuned_parameters = {'n_estimators':[10, 50, 100, 200, 500, 1000],'learning_rate':[0.00001, 0.001, 0.01, 0.1], 'algorithm':[\"SAMME\", \"SAMME.R\"]}\n",
        "        clf = GridSearchCV(\n",
        "            AdaBoostClassifier(), tuned_parameters, scoring='accuracy', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return accuracy_score(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "\n",
        "    if model == \"MLP\":\n",
        "        tuned_parameters = {\n",
        "            'hidden_layer_sizes': [(10,), (50,),(70,),(90,), (100,), (120,),(150,)],\n",
        "            'activation': ['tanh', 'relu'],\n",
        "            'solver': ['lbfgs', 'sgd', 'adam'],\n",
        "            'alpha': [0.0001, 0.01, 0.1],\n",
        "            'learning_rate': ['constant','adaptive'],\n",
        "            'max_iter':[100, 1000],\n",
        "        }\n",
        "        clf = GridSearchCV(\n",
        "            MLPClassifier(), tuned_parameters, scoring='accuracy', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return accuracy_score(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "\n",
        "    if model == \"NB\":\n",
        "        clf = GaussianNB()\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return accuracy_score(y_true, y_pred), None, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPJeec-i8-6G",
        "colab_type": "text"
      },
      "source": [
        "### **Run Free Text classification tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S32i1QCNmj8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(action='ignore', category=ConvergenceWarning)\n",
        "\n",
        "# function to call the compare_classification function for the specified model, feature_type and task\n",
        "def classification_results(problem, feature_type, model):\n",
        "    num_features = []\n",
        "    accuracy = []\n",
        "    hyper = []\n",
        "    val_score = []\n",
        "    for i in range(5,105,5):\n",
        "        res, setup, val = compare_classification(problem, feature_type, i, model)\n",
        "        num_features.append(i)\n",
        "        accuracy.append(res)\n",
        "        hyper.append(setup)\n",
        "        val_score.append(val)\n",
        "    print(num_features)\n",
        "    print(accuracy)\n",
        "    print(hyper)\n",
        "    #print(val_score)\n",
        "\n",
        "class_problems = [\"Gender\", \"Typing Style\", \"Major/Minor\"]\n",
        "models = [\"NB\", \"ABoost\", \"SVM\", \"DTree\", \"XGBoost\", \"MLP\"]\n",
        "\n",
        "for model in models:\n",
        "    print(\"###########################################################################################\")\n",
        "    print(model)\n",
        "    for class_problem in class_problems:\n",
        "        print(class_problem)\n",
        "        print(\"Desktop\")\n",
        "        classification_results(class_problem, get_desktop_features(), model)\n",
        "        print(\"Phone\")\n",
        "        classification_results(class_problem, get_phone_features(), model)\n",
        "        print(\"Tablet\")\n",
        "        classification_results(class_problem, get_tablet_features(), model)\n",
        "        print(\"Combined\")\n",
        "        classification_results(class_problem, get_combined_features(), model)\n",
        "        print()\n",
        "        print(\"-----------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll0ibOxx9EyE",
        "colab_type": "text"
      },
      "source": [
        "### **Run Fixed Text classification tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7XJVABt9IYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(action='ignore', category=ConvergenceWarning)\n",
        "\n",
        "# function to call the compare_classification function for the specified model, feature_type and task\n",
        "def classification_results(problem, feature_type, model):\n",
        "    num_features = []\n",
        "    accuracy = []\n",
        "    hyper = []\n",
        "    val_score = []\n",
        "    for i in range(5,105,5):\n",
        "        res, setup, val = compare_classification(problem, feature_type, i, model)\n",
        "        num_features.append(i)\n",
        "        accuracy.append(res)\n",
        "        hyper.append(setup)\n",
        "        val_score.append(val)\n",
        "    print(num_features)\n",
        "    print(accuracy)\n",
        "    print(hyper)\n",
        "    #print(val_score)\n",
        "\n",
        "class_problems = [\"Gender\", \"Typing Style\", \"Major/Minor\"]\n",
        "models = [\"NB\", \"SVM\", \"DTree\", \"ABoost\", \"MLP\", \"XGBoost\"]\n",
        "\n",
        "for model in models:\n",
        "    print(\"###########################################################################################\")\n",
        "    print(model)\n",
        "    for class_problem in class_problems:\n",
        "        print(class_problem)\n",
        "        print(\"Desktop\")\n",
        "        classification_results(class_problem, get_desktop_features_fixed(), model)\n",
        "        print(\"Phone\")\n",
        "        classification_results(class_problem, get_phone_features_fixed(), model)\n",
        "        print(\"Tablet\")\n",
        "        classification_results(class_problem, get_tablet_features_fixed(), model)\n",
        "        print(\"Combined\")\n",
        "        classification_results(class_problem, get_combined_features_fixed(), model)\n",
        "        print()\n",
        "        print(\"-----------------------------------------------------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Ej0JZomX1V",
        "colab_type": "text"
      },
      "source": [
        "# **Machine Learning Models: Regression tasks**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MErd5BerNGFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "def compare_regression(label_name, feature_type, top_n_features, model):\n",
        "''' Function to process the data and run the regression model specified using GridSearchCV\n",
        "Input:  label_name: The task to be performed (Age, Height)\n",
        "        feature_type: The feature set to be used (Desktop, Phone, Tablet, Combined)\n",
        "        top_n_features: Thu number of features to be selected using Mutual Info criterion\n",
        "        model: The ML model to train and evaluate\n",
        "Output: accuracy scores, best hyperparameters of the gridsearch run\n",
        "'''\n",
        "    # Creating class label vector using metadata\n",
        "    demographics_data_frame = pd.read_csv(\"Demographics.csv\")\n",
        "    Y_values = demographics_data_frame[label_name].to_numpy()\n",
        "\n",
        "    Y_vector = np.asarray(Y_values)\n",
        "\n",
        "    Y_vector = Y_vector[:-1]\n",
        "    Y_vector = Y_vector.astype('int')\n",
        "\n",
        "    X_matrix = feature_type\n",
        "\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_matrix = min_max_scaler.fit_transform(X_matrix)\n",
        "    X_matrix = preprocessing.scale(X_matrix)\n",
        "\n",
        "    np.random.seed(0)\n",
        "    X_matrix_new = SelectKBest(mutual_info_classif, k=top_n_features).fit_transform(X_matrix, Y_vector)\n",
        "\n",
        "    # Split the dataset in two equal parts\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\n",
        "    if model == \"SVM\":    \n",
        "        # Set the parameters by cross-validation\n",
        "        tuned_parameters = [{'kernel': ['rbf'], 'gamma': ['scale','auto'],\n",
        "                     'C': [0.1, 1, 10, 100, 1000]},\n",
        "                     {'kernel': ['poly'], 'gamma': ['scale','auto'],\n",
        "                     'C': [0.1, 1, 10, 100, 1000]},\n",
        "                     {'kernel': ['sigmoid'], 'gamma': ['scale','auto'],\n",
        "                     'C': [0.1, 1, 10, 100, 1000]},\n",
        "                    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}]\n",
        "\n",
        "        clf = GridSearchCV(\n",
        "            SVR(), tuned_parameters, scoring='neg_mean_absolute_error', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return mean_absolute_error(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "\n",
        "    if model == \"Lasso\":    \n",
        "        # Set the parameters by cross-validation\n",
        "        tuned_parameters = {\n",
        "            'alpha': [0.2, 0.4, 0.6, 0.8, 1],\n",
        "            'selection':['cyclic', 'random'],\n",
        "        }\n",
        "\n",
        "        clf = GridSearchCV(\n",
        "            Lasso(), tuned_parameters, scoring='neg_mean_absolute_error', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return mean_absolute_error(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "      \n",
        "    if model == \"Ridge\":    \n",
        "        # Set the parameters by cross-validation\n",
        "        tuned_parameters = {\n",
        "            'alpha': [25,10,4,2,1.0,0.8,0.5,0.3,0.2,0.1,0.05,0.02,0.01],\n",
        "        }\n",
        "\n",
        "        clf = GridSearchCV(\n",
        "            Ridge(), tuned_parameters, scoring='neg_mean_absolute_error', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return mean_absolute_error(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "    \n",
        "    if model == \"KNN\":    \n",
        "        # Set the parameters by cross-validation\n",
        "        tuned_parameters = {\n",
        "            'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 20],\n",
        "        }\n",
        "\n",
        "        clf = GridSearchCV(\n",
        "            KNeighborsRegressor(), tuned_parameters, scoring='neg_mean_absolute_error', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return mean_absolute_error(y_true, y_pred), clf.best_params_, clf.cv_results_\n",
        "\n",
        "    if model == \"XGB\":    \n",
        "        # Set the parameters by cross-validation\n",
        "        tuned_parameters = {\n",
        "              'objective':['reg:linear'],\n",
        "              'learning_rate': [.01, 0.1, .001], #so called `eta` value\n",
        "              'max_depth': [5, 10, 15, 20],\n",
        "              'min_child_weight': [4, 8],\n",
        "              'silent': [1],\n",
        "              'subsample': [0.7],\n",
        "              'colsample_bytree': [0.5, 0.7, 1.0],\n",
        "              'n_estimators': [100, 500, 800]}\n",
        "\n",
        "        clf = GridSearchCV(\n",
        "            XGBRegressor(), tuned_parameters, scoring='neg_mean_absolute_error', return_train_score=True\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_true, y_pred = y_test, clf.predict(X_test)\n",
        "        return mean_absolute_error(y_true, y_pred), clf.best_params_, clf.cv_results_    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2TzVPTzoDHS",
        "colab_type": "text"
      },
      "source": [
        "### **Run Free Text regression tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EL9OZKhNcMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(action='ignore', category=ConvergenceWarning)\n",
        "\n",
        "# function to call the compare_regression function for the specified model, feature_type and task\n",
        "def regression_results(problem, feature_type, model):\n",
        "    num_features = []\n",
        "    mae = []\n",
        "    hyper = []\n",
        "    val_score = []\n",
        "    for i in range(5,105,5):\n",
        "        res, setup, val = compare_regression(problem, feature_type, i, model)\n",
        "        num_features.append(i)\n",
        "        mae.append(res)\n",
        "        hyper.append(setup)\n",
        "        val_score.append(val)\n",
        "    print(num_features)\n",
        "    print(mae)\n",
        "    print(hyper)\n",
        "    #print(val_score)\n",
        "\n",
        "class_problems = [\"Age\", \"Height\"]\n",
        "models = [\"SVM\", \"KNN\", \"XGBoost\"]\n",
        "\n",
        "for model in models:\n",
        "    print(\"###########################################################################################\")\n",
        "    print(model)\n",
        "    for class_problem in class_problems:\n",
        "        print(class_problem)\n",
        "        print(\"Desktop\")\n",
        "        regression_results(class_problem, get_desktop_features(), model)\n",
        "        print(\"Phone\")\n",
        "        regression_results(class_problem, get_phone_features(), model)\n",
        "        print(\"Tablet\")\n",
        "        regression_results(class_problem, get_tablet_features(), model)\n",
        "        print(\"Combined\")\n",
        "        regression_results(class_problem, get_combined_features(), model)\n",
        "        print()\n",
        "        print(\"-----------------------------------------------------------------------------------------\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NKO_XbeoKzN",
        "colab_type": "text"
      },
      "source": [
        "### **Run Fixed Text regression tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3D75G95oQAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(action='ignore', category=ConvergenceWarning)\n",
        "\n",
        "# function to call the compare_regression function for the specified model, feature_type and task\n",
        "def regression_results(problem, feature_type, model):\n",
        "    num_features = []\n",
        "    mae = []\n",
        "    hyper = []\n",
        "    val_score = []\n",
        "    for i in range(5,105,5):\n",
        "        res, setup, val = compare_regression(problem, feature_type, i, model)\n",
        "        num_features.append(i)\n",
        "        mae.append(res)\n",
        "        hyper.append(setup)\n",
        "        val_score.append(val)\n",
        "    print(num_features)\n",
        "    print(mae)\n",
        "    print(hyper)\n",
        "    #print(val_score)\n",
        "\n",
        "class_problems = [\"Age\", \"Height\"]\n",
        "models = [\"SVM\", \"KNN\", \"XGBoost\"]\n",
        "\n",
        "for model in models:\n",
        "    print(\"###########################################################################################\")\n",
        "    print(model)\n",
        "    for class_problem in class_problems:\n",
        "        print(class_problem)\n",
        "        print(\"Desktop\")\n",
        "        regression_results(class_problem, get_desktop_features_fixed(), model)\n",
        "        print(\"Phone\")\n",
        "        regression_results(class_problem, get_phone_features_fixed(), model)\n",
        "        print(\"Tablet\")\n",
        "        regression_results(class_problem, get_tablet_features_fixed(), model)\n",
        "        print(\"Combined\")\n",
        "        regression_results(class_problem, get_combined_features_fixed(), model)\n",
        "        print()\n",
        "        print(\"-----------------------------------------------------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJenpwXq8mLn",
        "colab_type": "text"
      },
      "source": [
        "# **Deep Learning Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TaUSDBYa5rD",
        "colab_type": "text"
      },
      "source": [
        "## **Data Pre-Processing Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lvQQp7F4vtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the appropriate train-test splits for free text classification tasks to align with the ML models \n",
        "def get_train_test_splits(label_name):\n",
        "    demographics_data_frame = pd.read_csv(\"Demographics.csv\")\n",
        "    Y_values = demographics_data_frame[label_name].to_numpy()\n",
        "    Y_vector = np.asarray(Y_values)\n",
        "\n",
        "    if(label_name == \"Typing Style\"):\n",
        "        for i in range(117): \n",
        "            if(Y_vector[i] == 'a'):\n",
        "                Y_vector[i] = 0\n",
        "            elif(Y_vector[i] == 'b'):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 2\n",
        "\n",
        "    if(label_name == \"Major/Minor\"):\n",
        "        string1 = \"Computer\"\n",
        "        string2 = \"CS\"\n",
        "        for i, v in enumerate(Y_vector):\n",
        "            if (type(Y_vector[i]) == float):\n",
        "                Y_vector[i] = 1\n",
        "                continue\n",
        "            if \"LEFT\" in Y_vector[i]:\n",
        "                Y_vector[i] = 0\n",
        "            if string1 in v or string2 in v:\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    if (label_name == \"Gender\" or label_name == \"Ethnicity\"):\n",
        "        for i in range(116):\n",
        "            if(Y_values[i] == 'M' or Y_values[i] == \"Asian\"):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    Y_vector = Y_vector[:-1]\n",
        "    Y_values = Y_values[:-1]\n",
        "    Y_vector = Y_vector.astype('int')\n",
        "\n",
        "    #uncomment one of the below four lines for the required feature set\n",
        "    X_matrix = get_combined_features()\n",
        "    # X_matrix = get_desktop_features()\n",
        "    # X_matrix = get_phone_features()\n",
        "    # X_matrix = get_tablet_features()\n",
        "\n",
        "    # Normalizing features and selecting top 20\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_matrix = min_max_scaler.fit_transform(X_matrix)\n",
        "    X_matrix = preprocessing.scale(X_matrix)\n",
        "    np.random.seed(0)\n",
        "    X_matrix_new = X_matrix\n",
        "\n",
        "    print(X_matrix_new.shape)    \n",
        "    X_matrix_new, Y_vector = SMOTE(kind='svm').fit_sample(X_matrix_new, Y_vector)\n",
        "    return X_matrix_new, Y_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSlDAlFHPMof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the appropriate train-test splits for fixed text classification tasks to align with the ML models \n",
        "def get_train_test_splits_fixed(label_name):\n",
        "    demographics_data_frame = pd.read_csv(\"Demographics.csv\")\n",
        "    Y_values = demographics_data_frame[label_name].to_numpy()\n",
        "    Y_vector = np.asarray(Y_values)\n",
        "\n",
        "    if(label_name == \"Typing Style\"):\n",
        "        for i in range(117): \n",
        "            if(Y_vector[i] == 'a'):\n",
        "                Y_vector[i] = 0\n",
        "            elif(Y_vector[i] == 'b'):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 2\n",
        "\n",
        "    if(label_name == \"Major/Minor\"):\n",
        "        string1 = \"Computer\"\n",
        "        string2 = \"CS\"\n",
        "        for i, v in enumerate(Y_vector):\n",
        "            if (type(Y_vector[i]) == float):\n",
        "                Y_vector[i] = 1\n",
        "                continue\n",
        "            if \"LEFT\" in Y_vector[i]:\n",
        "                Y_vector[i] = 0\n",
        "            if string1 in v or string2 in v:\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    if (label_name == \"Gender\" or label_name == \"Ethnicity\"):\n",
        "        for i in range(116):\n",
        "            if(Y_values[i] == 'M' or Y_values[i] == \"Asian\"):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    Y_vector = Y_vector[:-1]\n",
        "    Y_values = Y_values[:-1]\n",
        "    Y_vector = Y_vector.astype('int')\n",
        "\n",
        "    # uncomment one of the below four lines for the required feature set\n",
        "    X_matrix = get_combined_features_fixed()\n",
        "    # X_matrix = get_desktop_features_fixed()\n",
        "    # X_matrix = get_phone_features_fixed()\n",
        "    # X_matrix = get_tablet_features_fixed()\n",
        "\n",
        "    # Normalizing features and selecting top 20\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_matrix = min_max_scaler.fit_transform(X_matrix)\n",
        "    X_matrix = preprocessing.scale(X_matrix)\n",
        "    np.random.seed(0)\n",
        "    X_matrix_new = X_matrix\n",
        "\n",
        "    print(X_matrix_new.shape)    \n",
        "\n",
        "    X_matrix_new, Y_vector = SMOTE(kind='svm').fit_sample(X_matrix_new, Y_vector)\n",
        "    return X_matrix_new, Y_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jtWhFcsEAYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the appropriate train-test splits for free text regression tasks to align with the ML models \n",
        "def get_train_test_splits_reg(label_name):\n",
        "    demographics_data_frame = pd.read_csv(\"Demographics.csv\")\n",
        "    Y_values = demographics_data_frame[label_name].to_numpy()\n",
        "    Y_vector = np.asarray(Y_values)\n",
        "\n",
        "    if(label_name == \"Typing Style\"):\n",
        "        for i in range(117): \n",
        "            if(Y_vector[i] == 'a'):\n",
        "                Y_vector[i] = 0\n",
        "            elif(Y_vector[i] == 'b'):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 2\n",
        "\n",
        "    if(label_name == \"Major/Minor\"):\n",
        "        string1 = \"Computer\"\n",
        "        string2 = \"CS\"\n",
        "        for i, v in enumerate(Y_vector):\n",
        "            if (type(Y_vector[i]) == float):\n",
        "                Y_vector[i] = 1\n",
        "                continue\n",
        "            if \"LEFT\" in Y_vector[i]:\n",
        "                Y_vector[i] = 0\n",
        "            if string1 in v or string2 in v:\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    if (label_name == \"Gender\" or label_name == \"Ethnicity\"):\n",
        "        for i in range(116):\n",
        "            if(Y_values[i] == 'M' or Y_values[i] == \"Asian\"):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    Y_vector = Y_vector[:-1]\n",
        "    Y_values = Y_values[:-1]\n",
        "    Y_vector = Y_vector.astype('int')\n",
        "\n",
        "    # uncomment one of the below four lines for the required feature set\n",
        "    X_matrix = get_combined_features()\n",
        "    # X_matrix = get_desktop_features()\n",
        "    # X_matrix = get_phone_features()\n",
        "    # X_matrix = get_tablet_features()\n",
        "\n",
        "    # Normalizing features and selecting top 20\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_matrix = min_max_scaler.fit_transform(X_matrix)\n",
        "    X_matrix = preprocessing.scale(X_matrix)\n",
        "    np.random.seed(0)\n",
        "    X_matrix_new = X_matrix\n",
        "\n",
        "    print(X_matrix_new.shape)    \n",
        "\n",
        "    return X_matrix_new, Y_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O3UBdDVPZkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the appropriate train-test splits for fixed text regression tasks to align with the ML models \n",
        "def get_train_test_splits_reg_fixed(label_name):\n",
        "    demographics_data_frame = pd.read_csv(\"Demographics.csv\")\n",
        "    Y_values = demographics_data_frame[label_name].to_numpy()\n",
        "    Y_vector = np.asarray(Y_values)\n",
        "\n",
        "    if(label_name == \"Typing Style\"):\n",
        "        for i in range(117): \n",
        "            if(Y_vector[i] == 'a'):\n",
        "                Y_vector[i] = 0\n",
        "            elif(Y_vector[i] == 'b'):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 2\n",
        "\n",
        "    if(label_name == \"Major/Minor\"):\n",
        "        string1 = \"Computer\"\n",
        "        string2 = \"CS\"\n",
        "        for i, v in enumerate(Y_vector):\n",
        "            if (type(Y_vector[i]) == float):\n",
        "                Y_vector[i] = 1\n",
        "                continue\n",
        "            if \"LEFT\" in Y_vector[i]:\n",
        "                Y_vector[i] = 0\n",
        "            if string1 in v or string2 in v:\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    if (label_name == \"Gender\" or label_name == \"Ethnicity\"):\n",
        "        for i in range(116):\n",
        "            if(Y_values[i] == 'M' or Y_values[i] == \"Asian\"):\n",
        "                Y_vector[i] = 1\n",
        "            else:\n",
        "                Y_vector[i] = 0\n",
        "\n",
        "    Y_vector = Y_vector[:-1]\n",
        "    Y_values = Y_values[:-1]\n",
        "    Y_vector = Y_vector.astype('int')\n",
        "\n",
        "    # uncomment one of the below four lines for the required feature set\n",
        "    X_matrix = get_combined_features_fixed()\n",
        "    # X_matrix = get_desktop_features_fixed()\n",
        "    # X_matrix = get_phone_features_fixed()\n",
        "    # X_matrix = get_tablet_features_fixed()\n",
        "\n",
        "    # Normalizing features and selecting top 20\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_matrix = min_max_scaler.fit_transform(X_matrix)\n",
        "    X_matrix = preprocessing.scale(X_matrix)\n",
        "    np.random.seed(0)\n",
        "    X_matrix_new = X_matrix\n",
        "\n",
        "    print(X_matrix_new.shape)    \n",
        "\n",
        "    return X_matrix_new, Y_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZS24kY30IW0",
        "colab_type": "text"
      },
      "source": [
        "## **FC network for classification/regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7jtaHIH0P90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A simple four layered NN for classification tasks\n",
        "class FC_Net(nn.Module):\n",
        "    def __init__(self, input_dims, num_classes):\n",
        "        super(FC_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dims, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 64)\n",
        "        self.fc4 = nn.Linear(64, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.dropout_2 = nn.Dropout(p=0.2)\n",
        "        self.dropout_5 = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        out = self.dropout_2(self.relu(self.fc1(feats)))\n",
        "        out = self.dropout_2(self.relu(self.fc2(out)))\n",
        "        out = self.dropout_2(self.relu(self.fc3(out)))\n",
        "        out = self.fc4(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lG4lLLH0TzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A simple four layered NN for regression tasks\n",
        "class Reg_FC_Net(nn.Module):\n",
        "    def __init__(self, input_dims):\n",
        "        super(Reg_FC_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dims,512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 64)\n",
        "        self.fc4 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.dropout_2 = nn.Dropout(p=0.2)\n",
        "        self.dropout_5 = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        out = self.relu(self.fc1(feats))\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.relu(self.fc3(out))\n",
        "        out = self.relu(self.fc4(out))\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orru8TyOPreU",
        "colab_type": "text"
      },
      "source": [
        "### **Run Free Text tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB7fA28Fby0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gender classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        " \n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num))\n",
        "\t\tsplit_num += 1 \n",
        "\n",
        "\t\tx_train, x_val = X_train[train_index], X_train[val_index]\n",
        "\t\ty_train, y_val = Y_train[train_index], Y_train[val_index]\n",
        "\n",
        "\t\tfcn = FC_Net(X_train.shape[1], 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(x_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(x_val)\n",
        "\t\tval_tensor_y = torch.Tensor(y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\t\t\t\t\t\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>50):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Gender\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bsvq7X04nCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Major/Minor classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        " \n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num))\n",
        "\t\tsplit_num += 1\n",
        "\n",
        "\t\tx_train, x_val = X_train[train_index], X_train[val_index]\n",
        "\t\ty_train, y_val = Y_train[train_index], Y_train[val_index]\n",
        "\n",
        "\t\tfcn = FC_Net(X_train.shape[1], 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(x_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(x_val)\n",
        "\t\tval_tensor_y = torch.Tensor(y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\t\t\t\t\t\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>60):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Major/Minor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcTBVU2d4pmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Typing Style classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        " \n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num))\n",
        "\t\tsplit_num += 1\n",
        "\n",
        "\t\tx_train, x_val = X_train[train_index], X_train[val_index]\n",
        "\t\ty_train, y_val = Y_train[train_index], Y_train[val_index]\n",
        "\n",
        "\t\tfcn = FC_Net(X_train.shape[1], 3)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(x_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(x_val)\n",
        "\t\tval_tensor_y = torch.Tensor(y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\t\t\t\t\t\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>20):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Typing Style\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hDeK8JZA4p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg(label_type)\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_FC_Net(X_train.shape[1])\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(40):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\t\t\t\t\t\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Age\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Lr6fR1A5z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Height regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg(label_type)\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_FC_Net(X_train.shape[1])\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(40):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\t\t\t\t\t\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Height\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rks_yYbEP4-e",
        "colab_type": "text"
      },
      "source": [
        "### **Run Fixed Text tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2Zk41wnP8jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gender classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        " \n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num))\n",
        "\t\tsplit_num += 1\n",
        "\n",
        "\t\tx_train, x_val = X_train[train_index], X_train[val_index]\n",
        "\t\ty_train, y_val = Y_train[train_index], Y_train[val_index]\n",
        "\n",
        "\t\tfcn = FC_Net(X_train.shape[1], 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(x_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(x_val)\n",
        "\t\tval_tensor_y = torch.Tensor(y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\t\t\t\t\t\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>60):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Gender\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r06IoH3FSTuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Major/Minor classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        " \n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num))\n",
        "\t\tsplit_num += 1\n",
        "\n",
        "\t\tx_train, x_val = X_train[train_index], X_train[val_index]\n",
        "\t\ty_train, y_val = Y_train[train_index], Y_train[val_index]\n",
        "\n",
        "\t\tfcn = FC_Net(X_train.shape[1], 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(x_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(x_val)\n",
        "\t\tval_tensor_y = torch.Tensor(y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\t\t\t\t\t\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>60):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Major/Minor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMjc4Y2EUC_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Typing Style classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        " \n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num))\n",
        "\t\tsplit_num += 1\n",
        "\n",
        "\t\tx_train, x_val = X_train[train_index], X_train[val_index]\n",
        "\t\ty_train, y_val = Y_train[train_index], Y_train[val_index]\n",
        "\n",
        "\t\tfcn = FC_Net(X_train.shape[1], 3)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(x_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(x_val)\n",
        "\t\tval_tensor_y = torch.Tensor(y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\t\t\t\t\t\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>60):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Typing Style\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8H3fz8FaVkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg_fixed(label_type)\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_FC_Net(X_train.shape[1])\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(40):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\t\t\t\t\t\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Age\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeYvBtLKdYM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Height regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg_fixed(label_type)\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_FC_Net(X_train.shape[1])\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(40):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\tl2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\treg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\t\t\t\t\t\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Height\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4svlZS8YRUnw",
        "colab_type": "text"
      },
      "source": [
        "## **CNN network for classification/regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poGU2vstR5LZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN model for classification tasks\n",
        "class CNN_Net(nn.Module):\n",
        "    def __init__(self, input_dims, num_classes):\n",
        "        super(CNN_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=2, bias=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, bias=True)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, bias=True)\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2, bias=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(1600, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.dropout_2 = nn.Dropout(p=0.2)\n",
        "        self.dropout_5 = nn.Dropout(p=0.5)\n",
        "        self.dropout_1 = nn.Dropout(p=0.1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, feats):\n",
        "        out = self.bn1(self.relu(self.conv1(feats)))\n",
        "        out = self.bn2(self.relu(self.conv2(out)))\n",
        "        out = self.bn3(self.relu(self.conv3(out)))\n",
        "        out = self.bn4(self.relu(self.conv4(out)))\n",
        "        out = out.view(-1, out.shape[1]*out.shape[2]*out.shape[3])\n",
        "        out = self.dropout_2(self.relu(self.fc1(out)))\n",
        "        out = self.dropout_2(self.relu(self.fc2(out)))\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "x = torch.randn((10, 1, 40, 40))\n",
        "CNN_Net(1, 2).forward(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOiYbwSnVH6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN model for regression tasks\n",
        "class Reg_CNN_Net(nn.Module):\n",
        "    def __init__(self, input_dims):\n",
        "        super(Reg_CNN_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=2, bias=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, bias=True)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, bias=True)\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2, bias=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(1600, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "        self.dropout_2 = nn.Dropout(p=0.2)\n",
        "        self.dropout_5 = nn.Dropout(p=0.5)\n",
        "        self.dropout_1 = nn.Dropout(p=0.1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.relu = nn.ReLU(); self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, feats):\n",
        "        out = self.bn1(self.relu(self.conv1(feats)))\n",
        "        out = self.bn2(self.relu(self.conv2(out)))\n",
        "        out = self.bn3(self.relu(self.conv3(out)))\n",
        "        out = self.bn4(self.relu(self.conv4(out)))\n",
        "        out = out.view(-1, out.shape[1]*out.shape[2]*out.shape[3])\n",
        "        out = self.dropout_2(self.relu(self.fc1(out)))\n",
        "        out = self.dropout_2(self.relu(self.fc2(out)))\n",
        "        # out = self.fc3(out)\n",
        "        out = self.sig(self.fc3(out))\n",
        "        return out*60\n",
        "\n",
        "x = torch.randn((10, 1, 40, 40))\n",
        "CNN_Net(1, 2).forward(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6ZSY9Hoqb1",
        "colab_type": "text"
      },
      "source": [
        "### **Run Free Text tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwWbQe1T6abf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gender classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num)); split_num += 1\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = CNN_Net(X_train.shape[1], 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>60):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Gender\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkGl2x176bnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Major/Minor classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num)); split_num += 1\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = CNN_Net(X_train.shape[1], 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>60):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Major/Minor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBD-pTi-6dD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Typing Style classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num)); split_num += 1\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = CNN_Net(X_train.shape[1], 3)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>10):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Typing Style\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0on6C0r_DSJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_CNN_Net(X_train.shape[1])\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\t\t\t\t\n",
        "\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)      \n",
        "\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)      \n",
        "\n",
        "\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Age\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjlBzwKODTff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_CNN_Net(X_train.shape[1])\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\t\t\t\t\n",
        "\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)      \n",
        "\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)      \n",
        "\n",
        "\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Height\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxbTxW-To0l9",
        "colab_type": "text"
      },
      "source": [
        "### **Run Fixed Text tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXfyO3oGo28H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gender classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num)); split_num += 1\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = CNN_Net(X_train.shape[1], 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>60):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Gender\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3s1qSIruyTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Major/Minor classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num)); split_num += 1\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = CNN_Net(X_train.shape[1], 3)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>60):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Major/Minor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOYkgY5swSNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Typing Style classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tprint('Split Num: '+str(split_num)); split_num += 1\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = CNN_Net(X_train.shape[1], 3)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>60):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Typing Style\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuD7EWNh6Wyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg_fixed(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_CNN_Net(X_train.shape[1])\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\t\t\t\t\n",
        "\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)      \n",
        "\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)      \n",
        "\n",
        "\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Age\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrOtnojw70NN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Height regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg_fixed(label_type)\n",
        "\tX_matrix_new = SelectKBest(mutual_info_classif, k=256).fit_transform(X_matrix_new, Y_vector)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 1, 40, 40))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_CNN_Net(X_train.shape[1])\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\t\t\t\t\n",
        "\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)      \n",
        "\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)      \n",
        "\n",
        "\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Height\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naqeW9Kuzub5",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM network for classification/regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qleEGdmPz7de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM model for classification tasks\n",
        "class LSTM_Net(nn.Module):\n",
        "    def __init__(self, batch_size, input_size, hidden_size, num_classes):\n",
        "        super(LSTM_Net, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True, num_layers=3)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, num_classes)\n",
        "\n",
        "        self.dropout_2 = nn.Dropout(p=0.2)\n",
        "        self.dropout_5 = nn.Dropout(p=0.5)\n",
        "        self.dropout_1 = nn.Dropout(p=0.1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, feats):        \n",
        "        h_0 = Variable(torch.zeros(3, self.batch_size, self.hidden_size))\n",
        "        c_0 = Variable(torch.zeros(3, self.batch_size, self.hidden_size))\n",
        "\n",
        "        h_0 = h_0.cuda()\n",
        "        c_0 = c_0.cuda()\n",
        "\n",
        "        out, (final_h, final_c) = self.lstm(feats, (h_0, c_0))\n",
        "        out = self.fc(final_h[-1])\n",
        "        return out\n",
        "\n",
        "LSTM_Net(10, 547, 10, 2).cuda().forward(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_bbrjWYKCVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM model for regression tasks\n",
        "class Reg_LSTM_Net(nn.Module):\n",
        "    def __init__(self, batch_size, input_size, hidden_size):\n",
        "        super(Reg_LSTM_Net, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True, num_layers=3)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "        self.dropout_2 = nn.Dropout(p=0.2)\n",
        "        self.dropout_5 = nn.Dropout(p=0.5)\n",
        "        self.dropout_1 = nn.Dropout(p=0.1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, feats):        \n",
        "        h_0 = Variable(torch.zeros(3, self.batch_size, self.hidden_size))\n",
        "        c_0 = Variable(torch.zeros(3, self.batch_size, self.hidden_size))\n",
        "\n",
        "        h_0 = h_0.cuda()\n",
        "        c_0 = c_0.cuda()\n",
        "\n",
        "        out, (final_h, final_c) = self.lstm(feats, (h_0, c_0))\n",
        "        out = self.relu(self.fc(final_h[-1]))\n",
        "        return out*100 #return out \n",
        "\n",
        "LSTM_Net(10, 547, 10, 2).cuda().forward(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qwuGXvcEEGe",
        "colab_type": "text"
      },
      "source": [
        "### **Run Free Text tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A2HjEus8wKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gender classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = LSTM_Net(10, 547, 10, 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>40):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Gender\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ypMUtb8z5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Major/Minor classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = LSTM_Net(10, 547, 10, 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>40):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Major/Minor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc6zCPrX82Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Typing style classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = LSTM_Net(10, 547, 10, 3)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>10):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Typing Style\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7NjoYf7EuPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_LSTM_Net(10, 547, 10)\n",
        "\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\t\t\t\t\t\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\t\t\t\t\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Age\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2cPyDE0Ev5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Height regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_LSTM_Net(10, 547, 10)\n",
        "\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\t\t\t\t\t\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\t\t\t\t\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Height\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCjPM93UEKex",
        "colab_type": "text"
      },
      "source": [
        "### **Run Fixed Text tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fyGkO_kENbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gender classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = LSTM_Net(10, 547, 10, 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>40):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Gender\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUfGt5LUGboD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Major/Minor classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = LSTM_Net(10, 547, 10, 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>40):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Major/Minor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRH1sUdtp1S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Typing Style classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = LSTM_Net(10, 547, 10, 3)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>40):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Typing Style\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF2CcsQ8r2M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_LSTM_Net(10, 547, 10)\n",
        "\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\t\t\t\t\t\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\t\t\t\t\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Age\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcBxKovPtcPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Height regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_LSTM_Net(10, 547, 10)\n",
        "\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\t\t\t\t\t\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\t\t\t\t\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Height\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MRUQAin-SxO",
        "colab_type": "text"
      },
      "source": [
        "## **RNN network for classification/regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvNBxY-B-aJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN model for classification tasks\n",
        "class RNN_Net(nn.Module):\n",
        "    def __init__(self, batch_size, input_size, hidden_size, num_classes):\n",
        "        super(RNN_Net, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True, num_layers=3)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, num_classes)\n",
        "\n",
        "        self.dropout_2 = nn.Dropout(p=0.2)\n",
        "        self.dropout_5 = nn.Dropout(p=0.5)\n",
        "        self.dropout_1 = nn.Dropout(p=0.1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, feats):   \n",
        "        h_0 = Variable(torch.zeros(3, self.batch_size, self.hidden_size))\n",
        "\n",
        "        h_0 = h_0.cuda()\n",
        "\n",
        "        out, final_h = self.rnn(feats, h_0)\n",
        "        out = self.fc(final_h[-1])\n",
        "        return out\n",
        "\n",
        "x = torch.randn((10, 3, 547)).cuda()\n",
        "RNN_Net(10, 547, 10, 2).cuda().forward(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZVqovz7LfoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN model for regression tasks\n",
        "class Reg_RNN_Net(nn.Module):\n",
        "    def __init__(self, batch_size, input_size, hidden_size):\n",
        "        super(Reg_RNN_Net, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True, num_layers=3)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "        self.dropout_2 = nn.Dropout(p=0.2)\n",
        "        self.dropout_5 = nn.Dropout(p=0.5)\n",
        "        self.dropout_1 = nn.Dropout(p=0.1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, feats):   \n",
        "        h_0 = Variable(torch.zeros(3, self.batch_size, self.hidden_size))\n",
        "\n",
        "        h_0 = h_0.cuda()\n",
        "\n",
        "        out, final_h = self.rnn(feats, h_0)\n",
        "        out = self.relu(self.fc(final_h[-1]))\n",
        "        return out*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQTbq6D41mnr",
        "colab_type": "text"
      },
      "source": [
        "### **Run Free Text tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQstLZFf-Lch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gender classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = RNN_Net(10, 547, 10, 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>=50):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Gender\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTJ9Ouwq-NNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Major/Minor classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = RNN_Net(10, 547, 10, 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>=50):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Major/Minor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D1jD_If-P75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Typing Style classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = RNN_Net(10, 547, 10, 3)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>=10):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Typing Style\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzUhrYbXbL6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_RNN_Net(10, 547, 10)\n",
        "\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<50):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Age\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtKfNB2cbOay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Height regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_RNN_Net(10, 547, 10)\n",
        "\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<100):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Height\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_AEyS-N1tw0",
        "colab_type": "text"
      },
      "source": [
        "### **Run Fixed Text tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LRUuoeb1wdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gender classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = RNN_Net(10, 547, 10, 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>=50):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Gender\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_R63qeq3eBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Major/Minor classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = RNN_Net(10, 547, 10, 2)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>=50):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Major/Minor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTsv8Z6e5PnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Typing Style classification\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = RNN_Net(10, 547, 10, 3)\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs, y.long())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_correct = 0\n",
        "\t\t\t\t\ttrain_total = 0\n",
        "\t\t\t\t\tval_correct = 0\n",
        "\t\t\t\t\tval_total = 0\n",
        "\t\t\t\t\ttest_correct = 0\n",
        "\t\t\t\t\ttest_total = 0\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttrain_total += y.size(0)\n",
        "\t\t\t\t\t\ttrain_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\tval_total += y.size(0)\n",
        "\t\t\t\t\t\tval_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\t\t\ttest_total += y.size(0)\n",
        "\t\t\t\t\t\ttest_correct += (y==predicted).sum().item()\n",
        "\n",
        "\t\t\t\tif(100*val_correct/val_total>=50):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Acc: '+str(100*train_correct/train_total)+', Val Acc: '+str(100*val_correct/val_total)+', Test Acc: '+str(100*test_correct/test_total))\n",
        "train_model(\"Typing Style\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNsFNl_M8bDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Age regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_RNN_Net(10, 547, 10)\n",
        "\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<50):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Age\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20nmiYohAlVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Height regression\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def weights_init(layer):\n",
        "\tif isinstance(layer, nn.Linear):\n",
        "\t\tlayer.bias.data.zero_()\n",
        "\t\tnn.init.kaiming_uniform_(layer.weight.data)\n",
        "\n",
        "def train_model(label_type):\n",
        "\tX_matrix_new, Y_vector = get_train_test_splits_reg_fixed(label_type)\n",
        "\tX_matrix_new = np.resize(X_matrix_new, (X_matrix_new.shape[0], 3, 547))\n",
        "\n",
        "\tX_train, X_test, Y_train, Y_test = train_test_split(X_matrix_new, Y_vector, test_size=0.3, random_state=0)\n",
        "\tkf = StratifiedKFold(n_splits=3)\n",
        "\tkf.get_n_splits(X_train)\n",
        "\n",
        "\tsplit_num = 0\n",
        "\n",
        "\tfor train_index, val_index in kf.split(X_train, Y_train):\n",
        "\t\tX_train, X_val = X_matrix_new[train_index], X_matrix_new[val_index]\n",
        "\t\tY_train, Y_val = Y_vector[train_index], Y_vector[val_index]\n",
        "\n",
        "\t\tfcn = Reg_RNN_Net(10, 547, 10)\n",
        "\n",
        "\t\tfcn.apply(weights_init)\n",
        "\t\toptimizer = torch.optim.Adam(fcn.parameters(), lr=0.001)\n",
        "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
        "\t\tloss_func = nn.MSELoss()\n",
        "\n",
        "\t\tfcn.cuda()\n",
        "\t\tloss_func.cuda()\n",
        "\n",
        "\t\ttrain_tensor_x = torch.Tensor(X_train)\n",
        "\t\ttrain_tensor_y = torch.Tensor(Y_train)\n",
        "\t\tval_tensor_x = torch.Tensor(X_val)\n",
        "\t\tval_tensor_y = torch.Tensor(Y_val)\n",
        "\t\ttest_tensor_x = torch.Tensor(X_test)\n",
        "\t\ttest_tensor_y = torch.Tensor(Y_test)\n",
        "\n",
        "\t\ttrain_dataset = torch.utils.data.TensorDataset(train_tensor_x, train_tensor_y)\n",
        "\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, drop_last=False)\n",
        "\n",
        "\t\tval_dataset = torch.utils.data.TensorDataset(val_tensor_x, val_tensor_y)\n",
        "\t\tval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\ttest_dataset = torch.utils.data.TensorDataset(test_tensor_x, test_tensor_y)\n",
        "\t\ttest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
        "\n",
        "\t\tfor epoch in range(10):\n",
        "\t\t\tfor itr, (x, y) in enumerate(train_dataloader):\n",
        "\t\t\t\tfcn.train()\n",
        "\t\t\t\tx = x.cuda()\n",
        "\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t# print(x.shape)\n",
        "\t\t\n",
        "\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\tloss = loss_func(outputs.view(-1), y.float())\n",
        "\n",
        "\t\t\t\tparams = list(fcn.parameters())\n",
        "\t\t\t\tl1_regularization, l2_regularization = torch.norm(params[0], 1), torch.norm(params[0], 2)\n",
        "\n",
        "\t\t\t\tfor param in params:\n",
        "\t\t\t\t\tl1_regularization += torch.norm(param, 1)\n",
        "\t\t\t\t\t# l2_regularization += torch.norm(param, 2)\n",
        "\n",
        "\t\t\t\treg_1 = Variable(l1_regularization)\n",
        "\t\t\t\t# reg_2 = Variable(l2_regularization)\n",
        "\n",
        "\t\t\t\tloss += reg_1\n",
        "\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tfcn.eval()\n",
        "\t\t\t\t\ttrain_array_preds = []\n",
        "\t\t\t\t\tval_array_preds = []\n",
        "\t\t\t\t\ttest_array_preds = []\n",
        "\t\t\t\t\ttrain_array_true = []\n",
        "\t\t\t\t\tval_array_true = []\n",
        "\t\t\t\t\ttest_array_true = []\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in train_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttrain_array_preds = train_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttrain_array_true = train_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in val_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\tval_array_preds = val_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\tval_array_true = val_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\t\tfor (x, y) in test_dataloader:\n",
        "\t\t\t\t\t\tx = x.cuda()\n",
        "\t\t\t\t\t\ty = y.cuda()\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tif(x.shape[0]!=10):\n",
        "\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\toutputs = fcn(x)\n",
        "\t\t\t\t\t\ttest_array_preds = test_array_preds + list(outputs.view(-1).detach().cpu().numpy())\n",
        "\t\t\t\t\t\ttest_array_true = test_array_true + list(y.float().detach().cpu().numpy())\n",
        "\n",
        "\t\t\t\ttrain_error = mean_absolute_error(train_array_true, train_array_preds)      \n",
        "\t\t\t\tval_error = mean_absolute_error(val_array_true, val_array_preds)\n",
        "\t\t\t\ttest_error = mean_absolute_error(test_array_true, test_array_preds)\n",
        "\n",
        "\t\t\t\tif(val_error<50):\n",
        "\t\t\t\t\tprint('Epoch: '+str(epoch)+', Itr: '+str(itr)+', Loss: '+str(loss.item())+', Train Error: '+str(train_error)+', Val Error: '+str(val_error)+', Test Error: '+str(test_error))\n",
        "train_model(\"Height\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}